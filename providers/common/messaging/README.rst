
 .. Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

 ..   http://www.apache.org/licenses/LICENSE-2.0

 .. Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.

 .. NOTE! THIS FILE IS AUTOMATICALLY GENERATED AND WILL BE OVERWRITTEN!

 .. IF YOU WANT TO MODIFY TEMPLATE FOR THIS FILE, YOU SHOULD MODIFY THE TEMPLATE
    `PROVIDER_README_TEMPLATE.rst.jinja2` IN the `dev/breeze/src/airflow_breeze/templates` DIRECTORY


Package ``apache-airflow-providers-common-msgq``

Release: ``0.1.0``



Provider package
----------------

This is a provider package for ``common.msgq`` provider. All classes for this provider package
are in ``airflow.providers.common.msgq`` python package.

This provider package is intended to serve as a common abstraction on top of popular message queue 
providers such as Apache Kafka, Amazon SQS, and Google PubSub. 

The expectation is that a common provider abstraction for message queues is especially useful for 
Event Driven Scheduling, which is being introduced as part of Airflow 3.0. Based on conversations 
with users of Apache Airflow, these Events for publishing of Data Assets are very often broadcast 
over a publish and subscribe mechanism. The underlying technology used for this publish and subscribe
mechanism varies by environment, but Apache Kafka, Amazon SQS, and Google PubSub are commonly used. 


Expected usage would be something on the lines below::
   
   trigger = = MsgQueueTrigger(msg_queue="https://sqs.us-east-1.amazonaws.com/722404908466/Test")
   
   data_asset = Asset("incoming_asset", watchers=[
       AssetWatcher(name="asset_watcher", trigger=trigger)
    )

    with DAG(
        dag_id="example_message_queue_asset",
        schedule=[data_asset],
        catchup=False,
    ):

        # Transform task which gets the data from sensor above and manipulates it with other data
        # 
        transform_task = TransformOperator(task_id="transform_task")

        # Publish task which takes the analyzed data and makes it available
        publish_task = PublishOperator(task_id="publish") 

        chain(tranform_task, publish_task)




.. You can find package information and changelog for the provider
.. in the `documentation <https://airflow.apache.org/docs/apache-airflow-providers-common-msgq/0.1.0/>`_.

Installation
------------

You can install this package on top of an existing Airflow 3 installation (see ``Requirements`` below
for the minimum Airflow version supported) via
``pip install apache-airflow-providers-common-msgq``

The package supports the following python versions: 3.10,3.11,3.12

Requirements
------------

==================  ==================
PIP package         Version required
==================  ==================
``apache-airflow``  ``>=3.0.0``
==================  ==================

Cross provider package dependencies
-----------------------------------

None at this time


The changelog for the provider package can be found in the
`changelog <https://airflow.apache.org/docs/apache-airflow-providers-common-msgq/0.1.0/changelog.html>`_.
