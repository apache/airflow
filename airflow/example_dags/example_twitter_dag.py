#-------------------------------------------------------------------------------------------------------------------------------------
# Caveat: This Dag will not run because of missing scripts.
#         The purpose of this is to give you a sample of a real world example DAG!
#-------------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------------
# Load The Dependencies
#-------------------------------------------------------------------------------------------------------------------------------------

import sys
reload(sys)
sys.setdefaultencoding("utf-8")
import time
from airflow import DAG
from airflow.operators import BashOperator
from airflow.operators import HiveOperator
from airflow.operators import PythonOperator
from datetime import datetime, timedelta
from airflow.hooks import HiveCliHook
from datetime import date, timedelta

#-------------------------------------------------------------------------------------------------------------------------------------
# Import Scripts for Python callable
#-------------------------------------------------------------------------------------------------------------------------------------

sys.path.append("python_scripts/example_twitter/")

from twitterapi import fetchtweet
from cleanapi import cleantweet
from analyzeapi import analyzetweet
from brokerapi import hivetomysql_tweet

#-------------------------------------------------------------------------------------------------------------------------------------
# set default arguments
#-------------------------------------------------------------------------------------------------------------------------------------

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2016, 3, 13),
    'email': ['airflow@airflow.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}

dag = DAG(
	'example_twitter_dag', default_args = default_args, 
	schedule_interval="@daily")
	
#-------------------------------------------------------------------------------------------------------------------------------------
# This task should call Twitter API and retrieve tweets from yesterday from and to for the four twitter users (Twitter_A,..,Twitter_D)
# There should be eight csv output files generated by this task and naming convention is direction(from or to)_twitterHandle_date.csv
#-------------------------------------------------------------------------------------------------------------------------------------

fetch_tweet = PythonOperator(
	task_id = 'fetch_tweet',
	python_callable = fetchtweet,
	dag = dag)
	
#-------------------------------------------------------------------------------------------------------------------------------------
# Clean the eight files. In this step you can get rid of or cherry pick columns and different parts of the text
#-------------------------------------------------------------------------------------------------------------------------------------

clean_tweet = PythonOperator(
        task_id = 'clean_tweet',
        python_callable = cleantweet,
        dag = dag)

clean_tweet.set_upstream(fetch_tweet)

#-------------------------------------------------------------------------------------------------------------------------------------
# In this section you can use a script to analyze the twitter data. Could simply be a sentiment analysis through algorithms like
# bag of words or something more complicated. You can also take a look at Web Services to do such tasks.
#-------------------------------------------------------------------------------------------------------------------------------------

analyze_tweet = PythonOperator(
	task_id = 'analyze_tweet',
	python_callable = analyzetweet,
	dag = dag)

analyze_tweet.set_upstream(clean_tweet)

#-------------------------------------------------------------------------------------------------------------------------------------
# Although this is the last task, we need to declare it before the next tasks as we will use set_downstream 
# This task will extract summary from Hive data and store it to MySQL
#-------------------------------------------------------------------------------------------------------------------------------------

hive_to_mysql = PythonOperator(
        task_id ='hive_to_mysql_hr',
        python_callable = hivetomysql_tweet,
	dag = dag)

#-------------------------------------------------------------------------------------------------------------------------------------
# The following tasks are generated using for loop. The first task puts the eight csv files to HDFS. The second task loads these files
# from HDFS to respected Hive tables. These two for loops could be combined into one loop. However, in most cases, you will be running
# different analysis on your incoming incoming and outgoing tweets, and hence they are kept seperated in this example.
#-------------------------------------------------------------------------------------------------------------------------------------

from_channels = ['fromTwitter_A','fromTwitter_B','fromTwitter_C','fromTwitter_D']
to_channels = ['toTwitter_A','toTwitter_B','toTwitter_C','toTwitter_D']

for channel in to_channels:
	yesterday = date.today()-timedelta(days=1)
	file_name = 'to_'+channel+'_'+yesterday.strftime("%Y-%m-%d")+'.csv'
	dt = yesterday.strftime("%Y-%m-%d")
	load_to_hdfs = BashOperator(
		task_id = 'put_'+channel+'_to_hdfs',
		bash_command = 'HADOOP_USER_NAME=hdfs hadoop fs -put -f /root/tweets/'+file_name+' /user/admin/tweets/'+channel+'/',
		dag = dag)
	
	load_to_hdfs.set_upstream(analyze_tweet)
	
	load_to_hive = HiveOperator(
		task_id = 'load_'+channel+'_to_hive',
		hql = "LOAD DATA INPATH " \
		"'/user/admin/tweets/"+channel+"/"+file_name+"' "\
		"INTO TABLE "+channel+" " \
		"PARTITION(dt='"+dt+"')",
		dag = dag)
	load_to_hive.set_upstream(load_to_hdfs)
	load_to_hive.set_downstream(hive_to_mysql)	

for channel in from_channels:
	yesterday = date.today()-timedelta(days=1)
	file_name = 'from_'+channel+'_'+yesterday.strftime("%Y-%m-%d")+'.csv'
	dt = yesterday.strftime("%Y-%m-%d")
	load_to_hdfs = BashOperator(
		task_id = 'put_'+channel+'_to_hdfs',
		bash_command = 'HADOOP_USER_NAME=hdfs hadoop fs -put -f /root/tweets/'+file_name+' /user/admin/tweets/'+channel+'/',
		dag = dag)
	
	load_to_hdfs.set_upstream(analyze_tweet)
	
	load_to_hive = HiveOperator(
		task_id = 'load_'+channel+'_to_hive',
		hql = "LOAD DATA INPATH " \
	"'/user/admin/tweets/"+channel+"/"+file_name+"' "\
		"INTO TABLE "+channel+" " \
		"PARTITION(dt='"+dt+"')",
		dag = dag)
		
	load_to_hive.set_upstream(load_to_hdfs)
	load_to_hive.set_downstream(hive_to_mysql)
