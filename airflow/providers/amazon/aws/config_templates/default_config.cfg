#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# This is the template for Airflow's default configuration. When Airflow is
# imported, it looks for a configuration file at $AIRFLOW_HOME/airflow.cfg. If
# it doesn't exist, Airflow uses this template to generate it by replacing
# variables in curly braces with their global values from configuration.py.

# Users should not modify this file; they should customize the generated
# airflow.cfg instead.


# ----------------------- TEMPLATE BEGINS HERE -----------------------

[batch]

# This section only applies if you are using the BatchExecutor in
# Airflow's ``[core]`` configuration.
# For more information on any of these execution parameters, see the link below:
# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch/client/submit_job.html
# For boto3 credential management, see
# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html
# The name of the AWS Region where AWS Batch is configured. Required.
# Example: region = us-east-1
# region =

# The name of the Airflow Job configured in AWS Batch. Required.
# Example: job_name = airflow-job-name
# job_name =

# The job queue where the Airflow job is submitted. You can specify either the name or the Amazon
# Resource Name (ARN) of the queue. Required.
# Example: job_queue = airflow-job-name or airflow-job-name:2
# job_queue =

# The name of the Airflow Job Queue configured in AWS Batch;
# optionally includes revision number. Required.
# job_definition =

# Points to a Python dictionary which provides **kwargs for boto3's Batch submit_job API. Most
# implementations can leave this to the default value. This API is provided for maximum extensibility
# for submitting Airflow Jobs on AWS Batch (i.e: retry strategy, timeouts, etc). More documentation
# can be found in the "Extensibility" section of the AWS Executors Readme.
# submit_job_kwargs =

[aws_ecs]

# This section only applies if you are using the AwsEcsExecutor in
# Airflow's ``[core]`` configuration.
# For more information on any of these execution parameters, see the link below:
# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs/client/run_task.html
# For boto3 credential management, see
# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html
# The name of the AWS Region where AWS ECS/Fargate is configured. Required.
# Example: region = us-east-1
# region =

# Name of AWS ECS or Fargate cluster. Required.
# cluster =

# Name of registered Airflow container within your AWS cluster. This container will
# receive an airflow CLI command as an additional parameter to its entrypoint.
# For more info see url to Boto3 docs above. Required.
# container_name =

# Name of AWS Task Definition. For more info see url to Boto3 docs above.
# Example: task_definition = us-east-1
# task_definition =

# Launch type can either be 'FARGATE' OR 'EC2'. For more info see url to Boto3 docs above.
platform_version = LATEST
launch_type = FARGATE

# Assign public ip. For more info see url to Boto3 docs above.
# assign_public_ip =

# Security group ids for task to run in (comma-separated). For more info see url to Boto3 docs above.
# security_groups =

# Subnets for task to run in (comma-separated). For more info see url to Boto3 docs above.
# Example: subnets = subnet-XXXXXXXX,subnet-YYYYYYYY
# subnets =

# This is the default configuration for calling the ECS `run_task` function API (see url above).
# To change the parameters used to run a task in FARGATE or ECS, the user can overwrite the path to
# specify another jinja-templated JSON. More documentation can be found in the DEFAULT_RUN_TASK_KWARGS
# variable in the AWS ECS Executor.
run_task_template = airflow.config_templates.default_aws_ecs.DEFAULT_AWS_ECS_CONFIG
