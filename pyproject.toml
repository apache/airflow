# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

[build-system]
# build dependencies should be fixed - including all transitive dependencies. This way we can ensure
# reproducibility of the build and make sure that any future releases of any dependencies will not
# break the build of released airflow sources in the future.
# The dependencies can be automatically upgraded by running:
# pre-commit run --hook-stage manual update-build-dependencies --all-files
requires = [
    "GitPython==3.1.41",
    "editables==0.5",
    "gitdb==4.0.11",
    "hatchling==1.21.1",
    "packaging==23.2",
    "pathspec==0.12.1",
    "pluggy==1.4.0",
    "smmap==5.0.1",
    "tomli==2.0.1; python_version < '3.11'",
    "trove-classifiers==2024.1.31",
]
build-backend = "hatchling.build"

[project]
name = "apache-airflow"
dynamic = ["version"]

description = "Programmatically author, schedule and monitor data pipelines"
readme = { file = "generated/PYPI_README.md", content-type = "text/markdown" }
license-files.globs = ["LICENSE", "3rd-party-licenses/*.txt"]
requires-python = "~=3.8"
authors = [
    { name = "Apache Software Foundation", email = "dev@airflow.apache.org" },
]
maintainers = [
    { name = "Apache Software Foundation", email="dev@airflow.apache.org" },
]
keywords = [ "airflow", "orchestration", "workflow", "dag", "pipelines", "automation", "data" ]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Environment :: Console",
    "Environment :: Web Environment",
    "Framework :: Apache Airflow",
    "Intended Audience :: Developers",
    "Intended Audience :: System Administrators",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: System :: Monitoring",
]
dependencies = [
    # Alembic is important to handle our migrations in predictable and performant way. It is developed
    # together with SQLAlchemy. Our experience with Alembic is that it very stable in minor version
    # The 1.13.0 of alembic marked some migration code as SQLAlchemy 2+ only so we limit it to 1.13.1
    "alembic>=1.13.1, <2.0",
    "argcomplete>=1.10",
    "asgiref",
    "attrs>=22.1.0",
    "blinker",
    # Colorlog 6.x merges TTYColoredFormatter into ColoredFormatter, breaking backwards compatibility with 4.x
    # Update CustomTTYColoredFormatter to remove
    "colorlog>=4.0.2, <5.0",
    "configupdater>=3.1.1",
    # `airflow/www/extensions/init_views` imports `connexion.decorators.validation.RequestBodyValidator`
    # connexion v3 has refactored the entire module to middleware, see: /spec-first/connexion/issues/1525
    # Specifically, RequestBodyValidator was removed in: /spec-first/connexion/pull/1595
    # The usage was added in #30596, seemingly only to override and improve the default error message.
    # Either revert that change or find another way, preferably without using connexion internals.
    # This limit can be removed after https://github.com/apache/airflow/issues/35234 is fixed
    "connexion[flask]>=2.10.0,<3.0",
    "cron-descriptor>=1.2.24",
    "croniter>=0.3.17",
    "cryptography>=0.9.3",
    "deprecated>=1.2.13",
    "dill>=0.2.2",
    "flask-caching>=1.5.0",
    # Flask-Session 0.6 add new arguments into the SqlAlchemySessionInterface constructor as well as
    # all parameters now are mandatory which make AirflowDatabaseSessionInterface incopatible with this version.
    "flask-session>=0.4.0,<0.6",
    "flask-wtf>=0.15",
    # Flask 2.3 is scheduled to introduce a number of deprecation removals - some of them might be breaking
    # for our dependencies - notably `_app_ctx_stack` and `_request_ctx_stack` removals.
    # We should remove the limitation after 2.3 is released and our dependencies are updated to handle it
    "flask>=2.2,<2.3",
    "fsspec>=2023.10.0",
    "google-re2>=1.0",
    "gunicorn>=20.1.0",
    "httpx",
    "importlib_metadata>=1.7;python_version<\"3.9\"",
    "importlib_resources>=5.2;python_version<\"3.9\"",
    "itsdangerous>=2.0",
    "jinja2>=3.0.0",
    "jsonschema>=4.18.0",
    "lazy-object-proxy",
    "linkify-it-py>=2.0.0",
    "lockfile>=0.12.2",
    "markdown-it-py>=2.1.0",
    "markupsafe>=1.1.1",
    "marshmallow-oneofschema>=2.0.1",
    "mdit-py-plugins>=0.3.0",
    "opentelemetry-api>=1.15.0",
    "opentelemetry-exporter-otlp",
    "packaging>=14.0",
    "pathspec>=0.9.0",
    "pendulum>=2.1.2,<4.0",
    "pluggy>=1.0",
    "psutil>=4.2.0",
    "pygments>=2.0.1",
    "pyjwt>=2.0.0",
    "python-daemon>=3.0.0",
    "python-dateutil>=2.3",
    "python-nvd3>=0.15.0",
    "python-slugify>=5.0",
    # Requests 3 if it will be released, will be heavily breaking.
    "requests>=2.27.0,<3",
    "rfc3339-validator>=0.1.4",
    "rich-argparse>=1.0.0",
    "rich>=12.4.4",
    "setproctitle>=1.1.8",
    # We use some deprecated features of sqlalchemy 2.0 and we should replace them before we can upgrade
    # See https://sqlalche.me/e/b8d9 for details of deprecated features
    # you can set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.
    # The issue tracking it is https://github.com/apache/airflow/issues/28723
    "sqlalchemy>=1.4.28,<2.0",
    "sqlalchemy-jsonfield>=1.0",
    "tabulate>=0.7.5",
    "tenacity>=6.2.0,!=8.2.0",
    "termcolor>=1.1.0",
    # We should remove this dependency when Providers are limited to Airflow 2.7+
    # as we replaced the usage of unicodecsv with csv in Airflow 2.7
    # See https://github.com/apache/airflow/pull/31693
    # We should also remove "licenses/LICENSE-unicodecsv.txt" file when we remove this dependency
    "unicodecsv>=0.14.1",
    # The Universal Pathlib provides  Pathlib-like interface for FSSPEC
    # In 0.1. *It was not very well defined for extension, so the way how we use it for 0.1.*
    # so we used a lot of private methods and attributes that were not defined in the interface
    # an they are broken with version 0.2.0 which is much better suited for extension and supports
    # Python 3.12. We should limit it, unti we migrate to 0.2.0
    # See: https://github.com/fsspec/universal_pathlib/pull/173#issuecomment-1937090528
    # This is prerequistite to make Airflow compatible with Python 3.12
    # Tracked in https://github.com/apache/airflow/pull/36755
    "universal-pathlib>=0.1.4,<0.2.0",
    # Werkzug 3 breaks Flask-Login 0.6.2, also connexion needs to be updated to >= 3.0
    # we should remove this limitation when FAB supports Flask 2.3 and we migrate connexion to 3+
    "werkzeug>=2.0,<3",
]

[project.optional-dependencies]
# Here manually managed extras start
# Those extras are manually managed and should be updated when needed
#
# START OF core extras
#
# This required for AWS deferrable operators.
# There is conflict between boto3 and aiobotocore dependency botocore.
# TODO: We can remove it once boto3 and aiobotocore both have compatible botocore version or
# boto3 have native aync support and we move away from aio aiobotocore
#
aiobotocore = [
    "aiobotocore>=2.7.0",
]
async = [
    "eventlet>=0.33.3",
    "gevent>=0.13",
    "greenlet>=0.4.9",
]
cgroups = [
    # Cgroupspy 0.2.2 added Python 3.10 compatibility
    "cgroupspy>=0.2.2",
]
deprecated-api = [
    "requests>=2.27.0,<3",
]
github-enterprise = [
    "apache-airflow[fab]",
    "authlib>=1.0.0",
]
google-auth = [
    "apache-airflow[fab]",
    "authlib>=1.0.0",
]
graphviz = [
    "graphviz>=0.12",
]
kerberos = [
    "pykerberos>=1.1.13",
    "requests-kerberos>=0.10.0",
    "thrift-sasl>=0.2.0",
]
ldap = [
    "ldap3>=2.5.1",
    "python-ldap",
]
leveldb = [
    "plyvel",
]
otel = [
    "opentelemetry-exporter-prometheus",
]
pandas = [
    "pandas>=1.2.5",
]
password = [
    "bcrypt>=2.0.0",
    "flask-bcrypt>=0.7.1",
]
pydantic = [
    "pydantic>=2.3.0",
]
rabbitmq = [
    "amqp",
]
s3fs = [
    # This is required for support of S3 file system which uses aiobotocore
    # which can have a conflict with boto3 as mentioned in aiobotocore extra
    "s3fs>=2023.10.0",
]
saml = [
    # This is required for support of SAML which might be used by some providers (e.g. Amazon)
    "python3-saml>=1.16.0",
]
sentry = [
    "blinker>=1.1",
    # Sentry SDK 1.33 is broken when greenlets are installed and fails to import
    # See https://github.com/getsentry/sentry-python/issues/2473
    "sentry-sdk>=1.32.0,!=1.33.0",
]
statsd = [
    "statsd>=3.3.0",
]
virtualenv = [
    "virtualenv",
]
# END OF core extras
# START OF Apache no provider extras
apache-atlas = [
    "atlasclient>=0.1.2",
]
apache-webhdfs = [
    "hdfs[avro,dataframe,kerberos]>=2.0.4",
]
# END OF Apache no provider extras
all-core = [
    "apache-airflow[aiobotocore]",
    "apache-airflow[apache-atlas]",
    "apache-airflow[async]",
    "apache-airflow[cgroups]",
    "apache-airflow[deprecated-api]",
    "apache-airflow[github-enterprise]",
    "apache-airflow[google-auth]",
    "apache-airflow[graphviz]",
    "apache-airflow[kerberos]",
    "apache-airflow[ldap]",
    "apache-airflow[leveldb]",
    "apache-airflow[otel]",
    "apache-airflow[pandas]",
    "apache-airflow[password]",
    "apache-airflow[pydantic]",
    "apache-airflow[rabbitmq]",
    "apache-airflow[s3fs]",
    "apache-airflow[saml]",
    "apache-airflow[sentry]",
    "apache-airflow[statsd]",
    "apache-airflow[apache-webhdfs]",
    "apache-airflow[virtualenv]",
]
# START OF devel extras
devel-debuggers = [
    "ipdb>=0.13.13",
]
devel-devscripts = [
    "click>=8.0",
    "gitpython>=3.1.40",
    "hatch>=1.9.1",
    "pipdeptree>=2.13.1",
    "pygithub>=2.1.1",
    "restructuredtext-lint>=1.4.0",
    "rich-click>=1.7.0",
    "semver>=3.0.2",
    "towncrier>=23.11.0",
    "twine>=4.0.2",
]
devel-duckdb = [
    "duckdb>=0.9.0",
]
# Mypy 0.900 and above ships only with stubs from stdlib so if we need other stubs, we need to install them
# manually as `types-*`. See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports
# for details. We want to install them explicitly because we want to eventually move to
# mypyd which does not support installing the types dynamically with --install-types
devel-mypy = [
    # TODO: upgrade to newer versions of MyPy continuously as they are released
    # Make sure to upgrade the mypy version in update-common-sql-api-stubs in .pre-commit-config.yaml
    # when you upgrade it here !!!!
    "mypy==1.8.0",
    "types-Deprecated",
    "types-Markdown",
    "types-PyMySQL",
    "types-PyYAML",
    "types-aiofiles",
    "types-certifi",
    "types-croniter",
    "types-docutils",
    "types-paramiko",
    "types-protobuf",
    "types-python-dateutil",
    "types-python-slugify",
    "types-pytz",
    "types-redis",
    "types-requests",
    "types-setuptools",
    "types-tabulate",
    "types-termcolor",
    "types-toml",
]
devel-sentry = [
    "blinker>=1.7.0",
]
devel-static-checks = [
    "black>=23.12.0",
    "pre-commit>=3.5.0",
    "ruff==0.2.1",
    "yamllint>=1.33.0",
]
devel-tests = [
    "aioresponses>=0.7.6",
    "backports.zoneinfo>=0.2.1;python_version<'3.9'",
    "beautifulsoup4>=4.7.1",
    "coverage>=7.2",
    "pytest-asyncio>=0.23.3",
    "pytest-cov>=4.1.0",
    "pytest-icdiff>=0.9",
    "pytest-instafail>=0.5.0",
    "pytest-mock>=3.12.0",
    "pytest-rerunfailures>=13.0",
    "pytest-timeouts>=1.2.1",
    "pytest-xdist>=3.5.0",
    # Temporary upper limmit to <8, not all dependencies at that moment ready to use 8.0
    # Internal meta-task for track https://github.com/apache/airflow/issues/37156
    "pytest>=7.4.4,<8.0",
    "requests_mock>=1.11.0",
    "time-machine>=2.13.0",
]
# END OF devel extras
# START OF doc extras
doc = [
    "astroid>=2.12.3,<3.0",
    "checksumdir>=1.2.0",
    # click 8.1.4 and 8.1.5 generate mypy errors due to typing issue in the upstream package:
    # https://github.com/pallets/click/issues/2558
    "click>=8.0,!=8.1.4,!=8.1.5",
    # Docutils 0.17.0 converts generated <div class="section"> into <section> and breaks our doc formatting
    # By adding a lot of whitespace separation. This limit can be lifted when we update our doc to handle
    # <section> tags for sections
    "docutils<0.17,>=0.16",
    "sphinx-airflow-theme>=0.0.12",
    "sphinx-argparse>=0.4.0",
    # sphinx-autoapi fails with astroid 3.0, see: https://github.com/readthedocs/sphinx-autoapi/issues/407
    # This was fixed in sphinx-autoapi 3.0, however it has requirement sphinx>=6.1, but we stuck on 5.x
    "sphinx-autoapi>=2.1.1",
    "sphinx-copybutton>=0.5.2",
    "sphinx-design>=0.5.0",
    "sphinx-jinja>=2.0.2",
    "sphinx-rtd-theme>=2.0.0",
    # Currently we are using sphinx 5 but we need to migrate to Sphinx 7
    "sphinx>=5.3.0,<6.0.0",
    "sphinxcontrib-applehelp>=1.0.4",
    "sphinxcontrib-devhelp>=1.0.2",
    "sphinxcontrib-htmlhelp>=2.0.1",
    "sphinxcontrib-httpdomain>=1.8.1",
    "sphinxcontrib-jquery>=4.1",
    "sphinxcontrib-jsmath>=1.0.1",
    "sphinxcontrib-qthelp>=1.0.3",
    "sphinxcontrib-redoc>=1.6.0",
    "sphinxcontrib-serializinghtml==1.1.5",
    "sphinxcontrib-spelling>=8.0.0",
]
doc-gen = [
    "apache-airflow[doc]",
    "eralchemy2>=1.3.8",
]
# END OF doc extras
# START OF bundle extras
all-dbs = [
    "apache-airflow[apache-cassandra]",
    "apache-airflow[apache-drill]",
    "apache-airflow[apache-druid]",
    "apache-airflow[apache-hdfs]",
    "apache-airflow[apache-hive]",
    "apache-airflow[apache-impala]",
    "apache-airflow[apache-pinot]",
    "apache-airflow[arangodb]",
    "apache-airflow[cloudant]",
    "apache-airflow[databricks]",
    "apache-airflow[exasol]",
    "apache-airflow[influxdb]",
    "apache-airflow[microsoft-mssql]",
    "apache-airflow[mongo]",
    "apache-airflow[mysql]",
    "apache-airflow[neo4j]",
    "apache-airflow[postgres]",
    "apache-airflow[presto]",
    "apache-airflow[trino]",
    "apache-airflow[vertica]",
]
devel = [
    "apache-airflow[celery]",
    "apache-airflow[cncf-kubernetes]",
    "apache-airflow[common-io]",
    "apache-airflow[common-sql]",
    "apache-airflow[devel-debuggers]",
    "apache-airflow[devel-devscripts]",
    "apache-airflow[devel-duckdb]",
    "apache-airflow[devel-mypy]",
    "apache-airflow[devel-sentry]",
    "apache-airflow[devel-static-checks]",
    "apache-airflow[devel-tests]",
    "apache-airflow[fab]",
    "apache-airflow[ftp]",
    "apache-airflow[http]",
    "apache-airflow[imap]",
    "apache-airflow[sqlite]",
]
devel-all-dbs = [
    "apache-airflow[apache-cassandra]",
    "apache-airflow[apache-drill]",
    "apache-airflow[apache-druid]",
    "apache-airflow[apache-hdfs]",
    "apache-airflow[apache-hive]",
    "apache-airflow[apache-impala]",
    "apache-airflow[apache-pinot]",
    "apache-airflow[arangodb]",
    "apache-airflow[cloudant]",
    "apache-airflow[databricks]",
    "apache-airflow[exasol]",
    "apache-airflow[influxdb]",
    "apache-airflow[microsoft-mssql]",
    "apache-airflow[mongo]",
    "apache-airflow[mysql]",
    "apache-airflow[neo4j]",
    "apache-airflow[postgres]",
    "apache-airflow[presto]",
    "apache-airflow[trino]",
    "apache-airflow[vertica]",
]
devel-ci = [
    "apache-airflow[devel-all]",
]
devel-hadoop = [
    "apache-airflow[apache-hdfs]",
    "apache-airflow[apache-hive]",
    "apache-airflow[apache-impala]",
    "apache-airflow[devel]",
    "apache-airflow[hdfs]",
    "apache-airflow[kerberos]",
    "apache-airflow[presto]",
]
# END OF bundle extras
#############################################################################################################
#  The whole section can be removed in Airflow 3.0 as those old aliases are deprecated in 2.* series
#############################################################################################################
# START OF deprecated extras
atlas = [
    "apache-airflow[apache-atlas]",
]
aws = [
    "apache-airflow[amazon]",
]
azure = [
    "apache-airflow[microsoft-azure]",
]
cassandra = [
    "apache-airflow[apache-cassandra]",
]
# Empty alias extra just for backward compatibility with Airflow 1.10
crypto = [
]
druid = [
    "apache-airflow[apache-druid]",
]
gcp = [
    "apache-airflow[google]",
]
gcp_api = [
    "apache-airflow[google]",
]
hdfs = [
    "apache-airflow[apache-hdfs]",
]
hive = [
    "apache-airflow[apache-hive]",
]
kubernetes = [
    "apache-airflow[cncf-kubernetes]",
]
mssql = [
    "apache-airflow[microsoft-mssql]",
]
pinot = [
    "apache-airflow[apache-pinot]",
]
s3 = [
    "apache-airflow[amazon]",
]
spark = [
    "apache-airflow[apache-spark]",
]
webhdfs = [
    "apache-airflow[apache-webhdfs]",
]
winrm = [
    "apache-airflow[microsoft-winrm]",
]
# END OF deprecated extras
#############################################################################################################
#  The whole section below is automatically generated by `update-providers-dependencies` pre-commit based
#  on `provider.yaml` files present in the `providers` subdirectories. The `provider.yaml` files are
#  A single source of truth for provider dependencies,
#
# PLEASE DO NOT MODIFY THIS SECTION MANUALLY. IT WILL BE OVERWRITTEN BY PRE-COMMIT !!
# If you want to modify these - modify the corresponding provider.yaml instead.
#############################################################################################################
# START OF GENERATED DEPENDENCIES
airbyte = [ # source: airflow/providers/airbyte/provider.yaml
  "apache-airflow[http]",
]
alibaba = [ # source: airflow/providers/alibaba/provider.yaml
  "alibabacloud_adb20211201>=1.0.0",
  "alibabacloud_tea_openapi>=0.3.7",
  "oss2>=2.14.0",
]
amazon = [ # source: airflow/providers/amazon/provider.yaml
  "PyAthena>=3.0.10",
  "apache-airflow[common_sql]",
  "apache-airflow[http]",
  "asgiref",
  "boto3>=1.33.0",
  "botocore>=1.33.0",
  "inflection>=0.5.1",
  "jsonpath_ng>=1.5.3",
  "redshift_connector>=2.0.918",
  "sqlalchemy_redshift>=0.8.6",
  "watchtower>=2.0.1,<4",
  # Devel dependencies for the amazon provider
  "aiobotocore>=2.7.0",
  "aws_xray_sdk>=2.12.0",
  "moto[cloudformation,glue]>=5.0.0",
  "mypy-boto3-appflow>=1.33.0",
  "mypy-boto3-rds>=1.33.0",
  "mypy-boto3-redshift-data>=1.33.0",
  "mypy-boto3-s3>=1.33.0",
  "s3fs>=2023.10.0",
]
apache-beam = [ # source: airflow/providers/apache/beam/provider.yaml
  "apache-beam>=2.53.0;python_version != \"3.12\"",
  "pyarrow>=14.0.1;python_version != \"3.12\"",
]
apache-cassandra = [ # source: airflow/providers/apache/cassandra/provider.yaml
  "cassandra-driver>=3.13.0",
]
apache-drill = [ # source: airflow/providers/apache/drill/provider.yaml
  "apache-airflow[common_sql]",
  "sqlalchemy-drill>=1.1.0",
]
apache-druid = [ # source: airflow/providers/apache/druid/provider.yaml
  "apache-airflow[common_sql]",
  "pydruid>=0.4.1",
]
apache-flink = [ # source: airflow/providers/apache/flink/provider.yaml
  "apache-airflow[cncf_kubernetes]",
  "cryptography>=2.0.0",
]
apache-hdfs = [ # source: airflow/providers/apache/hdfs/provider.yaml
  "hdfs[avro,dataframe,kerberos]>=2.0.4",
]
apache-hive = [ # source: airflow/providers/apache/hive/provider.yaml
  "apache-airflow[common_sql]",
  "hmsclient>=0.1.0",
  "pandas>=1.2.5",
  "pyhive[hive_pure_sasl]>=0.7.0",
  "thrift>=0.9.2",
]
apache-impala = [ # source: airflow/providers/apache/impala/provider.yaml
  "impyla>=0.18.0,<1.0",
]
apache-kafka = [ # source: airflow/providers/apache/kafka/provider.yaml
  "asgiref",
  "confluent-kafka>=1.8.2",
]
apache-kylin = [ # source: airflow/providers/apache/kylin/provider.yaml
  "kylinpy>=2.6",
]
apache-livy = [ # source: airflow/providers/apache/livy/provider.yaml
  "aiohttp>=3.9.2",
  "apache-airflow[http]",
  "asgiref",
]
apache-pig = [] # source: airflow/providers/apache/pig/provider.yaml
apache-pinot = [ # source: airflow/providers/apache/pinot/provider.yaml
  "apache-airflow[common_sql]",
  "pinotdb>0.4.7",
]
apache-spark = [ # source: airflow/providers/apache/spark/provider.yaml
  "grpcio-status>=1.59.0",
  "pyspark",
]
apprise = [ # source: airflow/providers/apprise/provider.yaml
  "apprise",
]
arangodb = [ # source: airflow/providers/arangodb/provider.yaml
  "python-arango>=7.3.2",
]
asana = [ # source: airflow/providers/asana/provider.yaml
  "asana>=0.10,<4.0.0",
]
atlassian-jira = [ # source: airflow/providers/atlassian/jira/provider.yaml
  "atlassian-python-api>=1.14.2,!=3.41.6",
  "beautifulsoup4",
]
celery = [ # source: airflow/providers/celery/provider.yaml
  "celery>=5.3.0,<6,!=5.3.3,!=5.3.2",
  "flower>=1.0.0",
  "google-re2>=1.0",
]
cloudant = [ # source: airflow/providers/cloudant/provider.yaml
  "cloudant>=2.0",
]
cncf-kubernetes = [ # source: airflow/providers/cncf/kubernetes/provider.yaml
  "aiofiles>=23.2.0",
  "asgiref>=3.5.2",
  "cryptography>=2.0.0",
  "google-re2>=1.0",
  "kubernetes>=28.1.0,<=29.0.0",
  "kubernetes_asyncio>=28.1.0,<=29.0.0",
]
cohere = [ # source: airflow/providers/cohere/provider.yaml
  "cohere>=4.37",
]
common-io = [] # source: airflow/providers/common/io/provider.yaml
common-sql = [ # source: airflow/providers/common/sql/provider.yaml
  "more-itertools>=9.0.0",
  "sqlparse>=0.4.2",
]
databricks = [ # source: airflow/providers/databricks/provider.yaml
  "aiohttp>=3.9.2, <4",
  "apache-airflow[common_sql]",
  "databricks-sql-connector>=2.0.0, <3.0.0, !=2.9.0",
  "requests>=2.27.0,<3",
  # Devel dependencies for the databricks provider
  "deltalake>=0.12.0",
]
datadog = [ # source: airflow/providers/datadog/provider.yaml
  "datadog>=0.14.0",
]
dbt-cloud = [ # source: airflow/providers/dbt/cloud/provider.yaml
  "aiohttp>=3.9.2",
  "apache-airflow[http]",
  "asgiref",
]
dingding = [ # source: airflow/providers/dingding/provider.yaml
  "apache-airflow[http]",
]
discord = [ # source: airflow/providers/discord/provider.yaml
  "apache-airflow[http]",
]
docker = [ # source: airflow/providers/docker/provider.yaml
  "docker>=5.0.3",
  "python-dotenv>=0.21.0",
]
elasticsearch = [ # source: airflow/providers/elasticsearch/provider.yaml
  "apache-airflow[common_sql]",
  "elasticsearch>=8.10,<9",
]
exasol = [ # source: airflow/providers/exasol/provider.yaml
  "apache-airflow[common_sql]",
  "pandas>=1.2.5",
  "pyexasol>=0.5.1",
]
fab = [ # source: airflow/providers/fab/provider.yaml
  "flask-appbuilder==4.3.11",
  "flask-login>=0.6.2",
  "flask>=2.2,<2.3",
  "google-re2>=1.0",
]
facebook = [ # source: airflow/providers/facebook/provider.yaml
  "facebook-business>=6.0.2",
]
ftp = [] # source: airflow/providers/ftp/provider.yaml
github = [ # source: airflow/providers/github/provider.yaml
  "PyGithub!=1.58",
]
google = [ # source: airflow/providers/google/provider.yaml
  "PyOpenSSL",
  "apache-airflow[common_sql]",
  "asgiref>=3.5.2",
  "gcloud-aio-auth>=4.0.0,<5.0.0",
  "gcloud-aio-bigquery>=6.1.2",
  "gcloud-aio-storage>=9.0.0",
  "gcsfs>=2023.10.0",
  "google-ads>=22.1.0",
  "google-analytics-admin",
  "google-api-core>=2.11.0,!=2.16.0",
  "google-api-python-client>=1.6.0",
  "google-auth-httplib2>=0.0.1",
  "google-auth>=1.0.0",
  "google-cloud-aiplatform>=1.22.1",
  "google-cloud-automl>=2.12.0",
  "google-cloud-batch>=0.13.0",
  "google-cloud-bigquery-datatransfer>=3.13.0",
  "google-cloud-bigtable>=2.17.0",
  "google-cloud-build>=3.22.0",
  "google-cloud-compute>=1.10.0",
  "google-cloud-container>=2.17.4",
  "google-cloud-datacatalog>=3.11.1",
  "google-cloud-dataflow-client>=0.8.6",
  "google-cloud-dataform>=0.5.0",
  "google-cloud-dataplex>=1.10.0",
  "google-cloud-dataproc-metastore>=1.12.0",
  "google-cloud-dataproc>=5.8.0",
  "google-cloud-dlp>=3.12.0",
  "google-cloud-kms>=2.15.0",
  "google-cloud-language>=2.9.0",
  "google-cloud-logging>=3.5.0",
  "google-cloud-memcache>=1.7.0",
  "google-cloud-monitoring>=2.18.0",
  "google-cloud-orchestration-airflow>=1.10.0",
  "google-cloud-os-login>=2.9.1",
  "google-cloud-pubsub>=2.19.0",
  "google-cloud-redis>=2.12.0",
  "google-cloud-run>=0.9.0",
  "google-cloud-secret-manager>=2.16.0",
  "google-cloud-spanner>=3.11.1",
  "google-cloud-speech>=2.18.0",
  "google-cloud-storage-transfer>=1.4.1",
  "google-cloud-storage>=2.7.0",
  "google-cloud-tasks>=2.13.0",
  "google-cloud-texttospeech>=2.14.1",
  "google-cloud-translate>=3.11.0",
  "google-cloud-videointelligence>=2.11.0",
  "google-cloud-vision>=3.4.0",
  "google-cloud-workflows>=1.10.0",
  "grpcio-gcp>=0.2.2",
  "httpx",
  "json-merge-patch>=0.2",
  "looker-sdk>=22.2.0",
  "pandas-gbq",
  "pandas>=1.2.5",
  "proto-plus>=1.19.6",
  "sqlalchemy-bigquery>=1.2.1",
  "sqlalchemy-spanner>=1.6.2",
]
grpc = [ # source: airflow/providers/grpc/provider.yaml
  "google-auth-httplib2>=0.0.1",
  "google-auth>=1.0.0, <3.0.0",
  "grpcio>=1.15.0",
]
hashicorp = [ # source: airflow/providers/hashicorp/provider.yaml
  "hvac>=1.1.0",
]
http = [ # source: airflow/providers/http/provider.yaml
  "aiohttp>=3.9.2",
  "asgiref",
  "requests>=2.27.0,<3",
  "requests_toolbelt",
]
imap = [] # source: airflow/providers/imap/provider.yaml
influxdb = [ # source: airflow/providers/influxdb/provider.yaml
  "influxdb-client>=1.19.0",
  "requests>=2.27.0,<3",
]
jdbc = [ # source: airflow/providers/jdbc/provider.yaml
  "apache-airflow[common_sql]",
  "jaydebeapi>=1.1.1",
]
jenkins = [ # source: airflow/providers/jenkins/provider.yaml
  "python-jenkins>=1.0.0",
]
microsoft-azure = [ # source: airflow/providers/microsoft/azure/provider.yaml
  "adal>=1.2.7",
  "adlfs>=2023.10.0",
  "azure-batch>=8.0.0",
  "azure-cosmos>=4.0.0",
  "azure-datalake-store>=0.0.45",
  "azure-identity>=1.3.1",
  "azure-keyvault-secrets>=4.1.0",
  "azure-kusto-data>=4.1.0",
  "azure-mgmt-containerinstance>=9.0.0",
  "azure-mgmt-containerregistry>=8.0.0",
  "azure-mgmt-cosmosdb",
  "azure-mgmt-datafactory>=2.0.0",
  "azure-mgmt-datalake-store>=0.5.0",
  "azure-mgmt-resource>=2.2.0",
  "azure-mgmt-storage>=16.0.0",
  "azure-servicebus>=7.6.1",
  "azure-storage-blob>=12.14.0",
  "azure-storage-file-datalake>=12.9.1",
  "azure-storage-file-share",
  "azure-synapse-artifacts>=0.17.0",
  "azure-synapse-spark",
  # Devel dependencies for the microsoft.azure provider
  "pywinrm",
]
microsoft-mssql = [ # source: airflow/providers/microsoft/mssql/provider.yaml
  "apache-airflow[common_sql]",
  "pymssql>=2.1.8",
]
microsoft-psrp = [ # source: airflow/providers/microsoft/psrp/provider.yaml
  "pypsrp>=0.8.0",
]
microsoft-winrm = [ # source: airflow/providers/microsoft/winrm/provider.yaml
  "pywinrm>=0.4",
]
mongo = [ # source: airflow/providers/mongo/provider.yaml
  "dnspython>=1.13.0",
  "pymongo>=3.6.0",
  # Devel dependencies for the mongo provider
  "mongomock",
]
mysql = [ # source: airflow/providers/mysql/provider.yaml
  "apache-airflow[common_sql]",
  "mysql-connector-python>=8.0.29",
  "mysqlclient>=1.3.6",
]
neo4j = [ # source: airflow/providers/neo4j/provider.yaml
  "neo4j>=4.2.1",
]
odbc = [ # source: airflow/providers/odbc/provider.yaml
  "apache-airflow[common_sql]",
  "pyodbc",
]
openai = [ # source: airflow/providers/openai/provider.yaml
  "openai[datalib]>=1.0",
]
openfaas = [] # source: airflow/providers/openfaas/provider.yaml
openlineage = [ # source: airflow/providers/openlineage/provider.yaml
  "apache-airflow[common_sql]",
  "attrs>=22.2",
  "openlineage-integration-common>=0.28.0",
  "openlineage-python>=0.28.0",
]
opensearch = [ # source: airflow/providers/opensearch/provider.yaml
  "opensearch-py>=2.2.0",
]
opsgenie = [ # source: airflow/providers/opsgenie/provider.yaml
  "opsgenie-sdk>=2.1.5",
]
oracle = [ # source: airflow/providers/oracle/provider.yaml
  "apache-airflow[common_sql]",
  "oracledb>=1.0.0",
]
pagerduty = [ # source: airflow/providers/pagerduty/provider.yaml
  "pdpyras>=4.1.2",
]
papermill = [ # source: airflow/providers/papermill/provider.yaml
  "ipykernel;python_version != \"3.12\"",
  "papermill[all]>=2.4.0;python_version != \"3.12\"",
  "scrapbook[all];python_version != \"3.12\"",
]
pgvector = [ # source: airflow/providers/pgvector/provider.yaml
  "apache-airflow[postgres]",
  "pgvector>=0.2.3",
]
pinecone = [ # source: airflow/providers/pinecone/provider.yaml
  "pinecone-client>=2.2.4,<3.0",
]
postgres = [ # source: airflow/providers/postgres/provider.yaml
  "apache-airflow[common_sql]",
  "psycopg2-binary>=2.8.0",
]
presto = [ # source: airflow/providers/presto/provider.yaml
  "apache-airflow[common_sql]",
  "pandas>=1.2.5",
  "presto-python-client>=0.8.4",
]
qdrant = [ # source: airflow/providers/qdrant/provider.yaml
  "qdrant_client>=1.7.0",
]
redis = [ # source: airflow/providers/redis/provider.yaml
  "redis>=4.5.2,<5.0.0,!=4.5.5",
]
salesforce = [ # source: airflow/providers/salesforce/provider.yaml
  "pandas>=1.2.5",
  "simple-salesforce>=1.0.0",
]
samba = [ # source: airflow/providers/samba/provider.yaml
  "smbprotocol>=1.5.0",
]
segment = [ # source: airflow/providers/segment/provider.yaml
  "analytics-python>=1.2.9",
]
sendgrid = [ # source: airflow/providers/sendgrid/provider.yaml
  "sendgrid>=6.0.0",
]
sftp = [ # source: airflow/providers/sftp/provider.yaml
  "apache-airflow[ssh]",
  "asyncssh>=2.12.0",
  "paramiko>=2.8.0",
]
singularity = [ # source: airflow/providers/singularity/provider.yaml
  "spython>=0.0.56",
]
slack = [ # source: airflow/providers/slack/provider.yaml
  "apache-airflow[common_sql]",
  "slack_sdk>=3.19.0",
]
smtp = [] # source: airflow/providers/smtp/provider.yaml
snowflake = [ # source: airflow/providers/snowflake/provider.yaml
  "apache-airflow[common_sql]",
  "snowflake-connector-python>=2.7.8",
  "snowflake-sqlalchemy>=1.1.0",
]
sqlite = [ # source: airflow/providers/sqlite/provider.yaml
  "apache-airflow[common_sql]",
]
ssh = [ # source: airflow/providers/ssh/provider.yaml
  "paramiko>=2.6.0",
  "sshtunnel>=0.3.2",
]
tableau = [ # source: airflow/providers/tableau/provider.yaml
  "tableauserverclient",
]
tabular = [ # source: airflow/providers/tabular/provider.yaml
  # Devel dependencies for the tabular provider
  "pyiceberg>=0.5.0",
]
telegram = [ # source: airflow/providers/telegram/provider.yaml
  "python-telegram-bot>=20.2",
]
teradata = [ # source: airflow/providers/teradata/provider.yaml
  "apache-airflow[common_sql]",
  "teradatasql>=17.20.0.28",
  "teradatasqlalchemy>=17.20.0.0",
]
trino = [ # source: airflow/providers/trino/provider.yaml
  "apache-airflow[common_sql]",
  "pandas>=1.2.5",
  "trino>=0.318.0",
]
vertica = [ # source: airflow/providers/vertica/provider.yaml
  "apache-airflow[common_sql]",
  "vertica-python>=0.5.1",
]
weaviate = [ # source: airflow/providers/weaviate/provider.yaml
  "pandas>=1.2.5",
  "weaviate-client>=3.24.2",
]
yandex = [ # source: airflow/providers/yandex/provider.yaml
  "yandexcloud>=0.228.0",
]
zendesk = [ # source: airflow/providers/zendesk/provider.yaml
  "zenpy>=2.0.24",
]
all = [
    # core extras
    "apache-airflow[aiobotocore]",
    "apache-airflow[async]",
    "apache-airflow[cgroups]",
    "apache-airflow[deprecated-api]",
    "apache-airflow[github-enterprise]",
    "apache-airflow[google-auth]",
    "apache-airflow[graphviz]",
    "apache-airflow[kerberos]",
    "apache-airflow[ldap]",
    "apache-airflow[leveldb]",
    "apache-airflow[otel]",
    "apache-airflow[pandas]",
    "apache-airflow[password]",
    "apache-airflow[pydantic]",
    "apache-airflow[rabbitmq]",
    "apache-airflow[s3fs]",
    "apache-airflow[saml]",
    "apache-airflow[sentry]",
    "apache-airflow[statsd]",
    "apache-airflow[virtualenv]",
    # Apache no provider extras
    "apache-airflow[apache-atlas]",
    "apache-airflow[apache-webhdfs]",
    "apache-airflow[all-core]",
    # Provider extras
    "apache-airflow[airbyte]",
    "apache-airflow[alibaba]",
    "apache-airflow[amazon]",
    "apache-airflow[apache-beam]",
    "apache-airflow[apache-cassandra]",
    "apache-airflow[apache-drill]",
    "apache-airflow[apache-druid]",
    "apache-airflow[apache-flink]",
    "apache-airflow[apache-hdfs]",
    "apache-airflow[apache-hive]",
    "apache-airflow[apache-impala]",
    "apache-airflow[apache-kafka]",
    "apache-airflow[apache-kylin]",
    "apache-airflow[apache-livy]",
    "apache-airflow[apache-pig]",
    "apache-airflow[apache-pinot]",
    "apache-airflow[apache-spark]",
    "apache-airflow[apprise]",
    "apache-airflow[arangodb]",
    "apache-airflow[asana]",
    "apache-airflow[atlassian-jira]",
    "apache-airflow[celery]",
    "apache-airflow[cloudant]",
    "apache-airflow[cncf-kubernetes]",
    "apache-airflow[cohere]",
    "apache-airflow[common-io]",
    "apache-airflow[common-sql]",
    "apache-airflow[databricks]",
    "apache-airflow[datadog]",
    "apache-airflow[dbt-cloud]",
    "apache-airflow[dingding]",
    "apache-airflow[discord]",
    "apache-airflow[docker]",
    "apache-airflow[elasticsearch]",
    "apache-airflow[exasol]",
    "apache-airflow[fab]",
    "apache-airflow[facebook]",
    "apache-airflow[ftp]",
    "apache-airflow[github]",
    "apache-airflow[google]",
    "apache-airflow[grpc]",
    "apache-airflow[hashicorp]",
    "apache-airflow[http]",
    "apache-airflow[imap]",
    "apache-airflow[influxdb]",
    "apache-airflow[jdbc]",
    "apache-airflow[jenkins]",
    "apache-airflow[microsoft-azure]",
    "apache-airflow[microsoft-mssql]",
    "apache-airflow[microsoft-psrp]",
    "apache-airflow[microsoft-winrm]",
    "apache-airflow[mongo]",
    "apache-airflow[mysql]",
    "apache-airflow[neo4j]",
    "apache-airflow[odbc]",
    "apache-airflow[openai]",
    "apache-airflow[openfaas]",
    "apache-airflow[openlineage]",
    "apache-airflow[opensearch]",
    "apache-airflow[opsgenie]",
    "apache-airflow[oracle]",
    "apache-airflow[pagerduty]",
    "apache-airflow[papermill]",
    "apache-airflow[pgvector]",
    "apache-airflow[pinecone]",
    "apache-airflow[postgres]",
    "apache-airflow[presto]",
    "apache-airflow[qdrant]",
    "apache-airflow[redis]",
    "apache-airflow[salesforce]",
    "apache-airflow[samba]",
    "apache-airflow[segment]",
    "apache-airflow[sendgrid]",
    "apache-airflow[sftp]",
    "apache-airflow[singularity]",
    "apache-airflow[slack]",
    "apache-airflow[smtp]",
    "apache-airflow[snowflake]",
    "apache-airflow[sqlite]",
    "apache-airflow[ssh]",
    "apache-airflow[tableau]",
    "apache-airflow[tabular]",
    "apache-airflow[telegram]",
    "apache-airflow[teradata]",
    "apache-airflow[trino]",
    "apache-airflow[vertica]",
    "apache-airflow[weaviate]",
    "apache-airflow[yandex]",
    "apache-airflow[zendesk]",
]
devel-all = [
    "apache-airflow[all]",
    "apache-airflow[devel]",
    "apache-airflow[doc]",
    "apache-airflow[doc-gen]",
    "apache-airflow[saml]",
    # Apache no provider extras
    "apache-airflow[apache-atlas]",
    "apache-airflow[apache-webhdfs]",
    "apache-airflow[all-core]",
    # Include all provider deps
    "apache-airflow[airbyte]",
    "apache-airflow[alibaba]",
    "apache-airflow[amazon]",
    "apache-airflow[apache-beam]",
    "apache-airflow[apache-cassandra]",
    "apache-airflow[apache-drill]",
    "apache-airflow[apache-druid]",
    "apache-airflow[apache-flink]",
    "apache-airflow[apache-hdfs]",
    "apache-airflow[apache-hive]",
    "apache-airflow[apache-impala]",
    "apache-airflow[apache-kafka]",
    "apache-airflow[apache-kylin]",
    "apache-airflow[apache-livy]",
    "apache-airflow[apache-pig]",
    "apache-airflow[apache-pinot]",
    "apache-airflow[apache-spark]",
    "apache-airflow[apprise]",
    "apache-airflow[arangodb]",
    "apache-airflow[asana]",
    "apache-airflow[atlassian-jira]",
    "apache-airflow[celery]",
    "apache-airflow[cloudant]",
    "apache-airflow[cncf-kubernetes]",
    "apache-airflow[cohere]",
    "apache-airflow[common-io]",
    "apache-airflow[common-sql]",
    "apache-airflow[databricks]",
    "apache-airflow[datadog]",
    "apache-airflow[dbt-cloud]",
    "apache-airflow[dingding]",
    "apache-airflow[discord]",
    "apache-airflow[docker]",
    "apache-airflow[elasticsearch]",
    "apache-airflow[exasol]",
    "apache-airflow[fab]",
    "apache-airflow[facebook]",
    "apache-airflow[ftp]",
    "apache-airflow[github]",
    "apache-airflow[google]",
    "apache-airflow[grpc]",
    "apache-airflow[hashicorp]",
    "apache-airflow[http]",
    "apache-airflow[imap]",
    "apache-airflow[influxdb]",
    "apache-airflow[jdbc]",
    "apache-airflow[jenkins]",
    "apache-airflow[microsoft-azure]",
    "apache-airflow[microsoft-mssql]",
    "apache-airflow[microsoft-psrp]",
    "apache-airflow[microsoft-winrm]",
    "apache-airflow[mongo]",
    "apache-airflow[mysql]",
    "apache-airflow[neo4j]",
    "apache-airflow[odbc]",
    "apache-airflow[openai]",
    "apache-airflow[openfaas]",
    "apache-airflow[openlineage]",
    "apache-airflow[opensearch]",
    "apache-airflow[opsgenie]",
    "apache-airflow[oracle]",
    "apache-airflow[pagerduty]",
    "apache-airflow[papermill]",
    "apache-airflow[pgvector]",
    "apache-airflow[pinecone]",
    "apache-airflow[postgres]",
    "apache-airflow[presto]",
    "apache-airflow[qdrant]",
    "apache-airflow[redis]",
    "apache-airflow[salesforce]",
    "apache-airflow[samba]",
    "apache-airflow[segment]",
    "apache-airflow[sendgrid]",
    "apache-airflow[sftp]",
    "apache-airflow[singularity]",
    "apache-airflow[slack]",
    "apache-airflow[smtp]",
    "apache-airflow[snowflake]",
    "apache-airflow[sqlite]",
    "apache-airflow[ssh]",
    "apache-airflow[tableau]",
    "apache-airflow[tabular]",
    "apache-airflow[telegram]",
    "apache-airflow[teradata]",
    "apache-airflow[trino]",
    "apache-airflow[vertica]",
    "apache-airflow[weaviate]",
    "apache-airflow[yandex]",
    "apache-airflow[zendesk]",
]
# END OF GENERATED DEPENDENCIES
#############################################################################################################
#  The rest of the pyproject.toml file should be manually maintained
#############################################################################################################
[project.scripts]
airflow = "airflow.__main__:main"
[project.urls]
"Bug Tracker" = "https://github.com/apache/airflow/issues"
Documentation = "https://airflow.apache.org/docs/"
Downloads = "https://archive.apache.org/dist/airflow/"
Homepage = "https://airflow.apache.org/"
"Release Notes" = "https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html"
"Slack Chat" = "https://s.apache.org/airflow-slack"
"Source Code" = "https://github.com/apache/airflow"
Twitter = "https://twitter.com/ApacheAirflow"
YouTube = "https://www.youtube.com/channel/UCSXwxpWZQ7XZ1WL3wqevChA/"

[tool.hatch.envs.default]
python = "3.8"
platforms = ["linux", "macos"]
description = "Default environment with Python 3.8 for maximum compatibility"
features = ["devel"]

[tool.hatch.envs.airflow-38]
python = "3.8"
platforms = ["linux", "macos"]
description = "Environment with Python 3.8. No devel installed."
features = []

[tool.hatch.envs.airflow-39]
python = "3.9"
platforms = ["linux", "macos"]
description = "Environment with Python 3.9. No devel installed."
features = []

[tool.hatch.envs.airflow-310]
python = "3.10"
platforms = ["linux", "macos"]
description = "Environment with Python 3.10. No devel installed."
features = []

[tool.hatch.envs.airflow-311]
python = "3.11"
platforms = ["linux", "macos"]
description = "Environment with Python 3.11. No devel installed"
features = []

[tool.hatch.version]
path = "airflow/__init__.py"

[tool.hatch.build.targets.wheel.hooks.custom]
path = "./hatch_build.py"

[tool.hatch.build.hooks.custom]
path = "./hatch_build.py"

[tool.hatch.build.targets.custom]
path = "./hatch_build.py"

[tool.hatch.build.targets.sdist]
include = [
    "/airflow",
    "/airflow/git_version"
]
exclude = [
    "/airflow/providers/",
    "/airflow/www/node_modules/"
]
artifacts = [
    "/airflow/www/static/dist/",
    "/airflow/git_version",
    "/generated/",
    "airflow_pre_installed_providers.txt",
]


[tool.hatch.build.targets.wheel]
include = [
    "/airflow",
]
exclude = [
    "/airflow/providers/",
]
artifacts = [
    "/airflow/www/static/dist/",
    "/airflow/git_version"
]

## black settings ##
[tool.black]
line-length = 110
target-version = ['py38', 'py39', 'py310', 'py311']


## ruff settings ##
[tool.ruff]
target-version = "py38"
line-length = 110
extend-exclude = [
    ".eggs",
    "airflow/_vendor/*",
    "airflow/providers/google/ads/_vendor/*",
    # The files generated by stubgen aren't 100% valid syntax it turns out, and we don't ship them, so we can
    # ignore them in ruff
    "airflow/providers/common/sql/*/*.pyi",
    "airflow/migrations/versions/*.py",
    "tests/dags/test_imports.py",
]

namespace-packages = ["airflow/providers"]

[tool.ruff.lint]
typing-modules = ["airflow.typing_compat"]
extend-select = [
    "I", # Missing required import (auto-fixable)
    "UP", # Pyupgrade
    "RUF100", # Unused noqa (auto-fixable)
    # We ignore more pydocstyle than we enable, so be more selective at what we enable
    # We add modules that do not follow the rule `Missing docstring in public module`,
    # `Missing docstring in public method`, `Missing docstring in public package`,
    # `Missing docstring in magic method`, `Missing docstring in __init__`
    # and should remove it from that list as soon as it follows.
    # See: https://github.com/apache/airflow/issues/10742
    "D100",
    "D101",
    "D102",
    "D103",
    "D104",
    "D105",
    "D106",
    "D2",
    "D3",
    "D400",
    "D401",
    "D402",
    "D403",
    "D412",
    "D419",
    "TCH",  # Rules around TYPE_CHECKING blocks
    "TID251",  # Specific modules or module members that may not be imported or accessed
    "TID253",  # Ban certain modules from being imported at module level
    "ISC",  # Checks for implicit literal string concatenation (auto-fixable)
    "B006", # Checks for uses of mutable objects as function argument defaults.
    "PT014", # Checks for duplicate test cases in pytest.mark.parametrize
]
ignore = [
    "D203",
    "D212",
    "D213",
    "D214",
    "D215",
    "E731",
    "TCH003",  # Do not move imports from stdlib to TYPE_CHECKING block
]

[tool.ruff.format]
docstring-code-format = true

[tool.ruff.lint.isort]
required-imports = ["from __future__ import annotations"]
combine-as-imports = true

[tool.ruff.lint.per-file-ignores]
"airflow/models/__init__.py" = ["F401", "TCH004"]
"airflow/models/sqla_models.py" = ["F401"]

# The test_python.py is needed because adding __future__.annotations breaks runtime checks that are
# needed for the test to work
"tests/decorators/test_python.py" = ["I002"]

# The Pydantic representations of SqlAlchemy Models are not parsed well with Pydantic
# when __future__.annotations is used so we need to skip them from upgrading
# Pydantic also require models to be imported during execution
"airflow/serialization/pydantic/*.py" = ["I002", "UP007", "TCH001"]

# Ignore pydoc style from these
"*.pyi" = ["D"]
"scripts/*" = ["D"]
"docs/*" = ["D"]
"provider_packages/*" = ["D"]
"*/example_dags/*" = ["D"]
"chart/*" = ["D"]
"dev/*" = ["D"]
# In addition ignore top level imports, e.g. pandas, numpy in tests
"dev/perf/*" = ["TID253"]
"dev/breeze/tests/*" = ["TID253"]
"tests/*" = ["D", "TID253"]
"docker_tests/*" = ["D", "TID253"]
"kubernetes_tests/*" = ["D", "TID253"]
"helm_tests/*" = ["D", "TID253"]

# All of the modules which have an extra license header (i.e. that we copy from another project) need to
# ignore E402 -- module level import not at top level
"scripts/ci/pre_commit/*.py" = ["E402"]
"airflow/api/auth/backend/kerberos_auth.py" = ["E402"]
"airflow/security/kerberos.py" = ["E402"]
"airflow/security/utils.py" = ["E402"]
"tests/providers/elasticsearch/log/elasticmock/__init__.py" = ["E402"]
"tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py" = ["E402"]
"tests/providers/openai/hooks/test_openai.py" = ["E402"]
"tests/providers/openai/operators/test_openai.py" = ["E402"]
"tests/providers/qdrant/hooks/test_qdrant.py" = ["E402"]
"tests/providers/qdrant/operators/test_qdrant.py" = ["E402"]

# All the modules which do not follow D100-D105 yet, please remove as soon as it becomes compatible
"airflow/__main__.py" = ["D103"]
"airflow/api/auth/__init__.py" = ["D104"]
"airflow/api/auth/backend/__init__.py" = ["D104"]
"airflow/api/auth/backend/basic_auth.py" = ["D103"]
"airflow/api/client/local_client.py" = ["D102"]
"airflow/api/common/__init__.py" = ["D104"]
"airflow/api/common/airflow_health.py" = ["D100"]
"airflow/api_connexion/__init__.py" = ["D104"]
"airflow/api_connexion/endpoints/__init__.py" = ["D104"]
"airflow/api_connexion/endpoints/config_endpoint.py" = ["D100", "D103"]
"airflow/api_connexion/endpoints/connection_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/dag_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/dag_run_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/dag_source_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/dag_warning_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/dataset_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/event_log_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/extra_link_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/forward_to_fab_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/health_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/import_error_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/log_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/plugin_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/pool_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/provider_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/request_dict.py" = ["D100"]
"airflow/api_connexion/endpoints/task_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/task_instance_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/update_mask.py" = ["D100", "D103"]
"airflow/api_connexion/endpoints/variable_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/version_endpoint.py" = ["D100"]
"airflow/api_connexion/endpoints/xcom_endpoint.py" = ["D100"]
"airflow/api_connexion/exceptions.py" = ["D100"]
"airflow/api_connexion/parameters.py" = ["D100"]
"airflow/api_connexion/schemas/__init__.py" = ["D104"]
"airflow/api_connexion/schemas/common_schema.py" = ["D100"]
"airflow/api_connexion/schemas/config_schema.py" = ["D100"]
"airflow/api_connexion/schemas/connection_schema.py" = ["D100", "D102"]
"airflow/api_connexion/schemas/dag_run_schema.py" = ["D100", "D102"]
"airflow/api_connexion/schemas/dag_schema.py" = ["D100", "D102"]
"airflow/api_connexion/schemas/dag_source_schema.py" = ["D100"]
"airflow/api_connexion/schemas/dag_warning_schema.py" = ["D100"]
"airflow/api_connexion/schemas/dataset_schema.py" = ["D100"]
"airflow/api_connexion/schemas/enum_schemas.py" = ["D100"]
"airflow/api_connexion/schemas/error_schema.py" = ["D100"]
"airflow/api_connexion/schemas/event_log_schema.py" = ["D100"]
"airflow/api_connexion/schemas/health_schema.py" = ["D100"]
"airflow/api_connexion/schemas/job_schema.py" = ["D100"]
"airflow/api_connexion/schemas/log_schema.py" = ["D100"]
"airflow/api_connexion/schemas/plugin_schema.py" = ["D100"]
"airflow/api_connexion/schemas/pool_schema.py" = ["D100"]
"airflow/api_connexion/schemas/provider_schema.py" = ["D100"]
"airflow/api_connexion/schemas/role_and_permission_schema.py" = ["D100"]
"airflow/api_connexion/schemas/sla_miss_schema.py" = ["D100"]
"airflow/api_connexion/schemas/task_instance_schema.py" = ["D100", "D102"]
"airflow/api_connexion/schemas/task_schema.py" = ["D100"]
"airflow/api_connexion/schemas/trigger_schema.py" = ["D100"]
"airflow/api_connexion/schemas/user_schema.py" = ["D100"]
"airflow/api_connexion/schemas/variable_schema.py" = ["D100"]
"airflow/api_connexion/schemas/version_schema.py" = ["D100"]
"airflow/api_connexion/schemas/xcom_schema.py" = ["D100"]
"airflow/api_connexion/security.py" = ["D100", "D103"]
"airflow/api_connexion/types.py" = ["D100"]
"airflow/api_internal/__init__.py" = ["D104"]
"airflow/api_internal/endpoints/__init__.py" = ["D104"]
"airflow/api_internal/endpoints/rpc_api_endpoint.py" = ["D100"]
"airflow/api_internal/internal_api_call.py" = ["D100", "D102"]
"airflow/auth/__init__.py" = ["D104"]
"airflow/auth/managers/__init__.py" = ["D104"]
"airflow/auth/managers/base_auth_manager.py" = ["D100"]
"airflow/auth/managers/fab/__init__.py" = ["D104"]
"airflow/auth/managers/fab/api/__init__.py" = ["D104"]
"airflow/auth/managers/fab/api/auth/__init__.py" = ["D104"]
"airflow/auth/managers/fab/api/auth/backend/__init__.py" = ["D104"]
"airflow/auth/managers/fab/api/auth/backend/basic_auth.py" = ["D103"]
"airflow/auth/managers/fab/security_manager/__init__.py" = ["D104"]
"airflow/auth/managers/fab/security_manager/override.py" = ["D100"]
"airflow/auth/managers/models/__init__.py" = ["D104"]
"airflow/auth/managers/models/base_user.py" = ["D100", "D102"]
"airflow/auth/managers/models/batch_apis.py" = ["D100"]
"airflow/auth/managers/models/resource_details.py" = ["D100"]
"airflow/auth/managers/utils/__init__.py" = ["D104"]
"airflow/auth/managers/utils/fab.py" = ["D100"]
"airflow/callbacks/__init__.py" = ["D104"]
"airflow/callbacks/base_callback_sink.py" = ["D100"]
"airflow/callbacks/callback_requests.py" = ["D100", "D102", "D105"]
"airflow/callbacks/database_callback_sink.py" = ["D100"]
"airflow/callbacks/pipe_callback_sink.py" = ["D100"]
"airflow/cli/__init__.py" = ["D104"]
"airflow/cli/cli_parser.py" = ["D102"]
"airflow/cli/commands/__init__.py" = ["D104"]
"airflow/cli/commands/cheat_sheet_command.py" = ["D100"]
"airflow/cli/commands/connection_command.py" = ["D103"]
"airflow/cli/commands/daemon_utils.py" = ["D100"]
"airflow/cli/commands/info_command.py" = ["D102"]
"airflow/cli/commands/jobs_command.py" = ["D100"]
"airflow/cli/commands/legacy_commands.py" = ["D100"]
"airflow/cli/commands/plugins_command.py" = ["D100"]
"airflow/cli/commands/provider_command.py" = ["D103"]
"airflow/cli/commands/standalone_command.py" = ["D100", "D102"]
"airflow/cli/commands/task_command.py" = ["D102", "D105"]
"airflow/cli/commands/triggerer_command.py" = ["D103"]
"airflow/cli/simple_table.py" = ["D100", "D103"]
"airflow/cli/utils.py" = ["D100", "D103"]
"airflow/compat/__init__.py" = ["D104"]
"airflow/compat/functools.py" = ["D100"]
"airflow/config_templates/__init__.py" = ["D104"]
"airflow/configuration.py" = ["D100", "D102", "D103", "D105"]
"airflow/dag_processing/__init__.py" = ["D104"]
"airflow/dag_processing/manager.py" = ["D102", "D103"]
"airflow/dag_processing/processor.py" = ["D100", "D102"]
"airflow/datasets/__init__.py" = ["D104", "D105"]
"airflow/datasets/manager.py" = ["D100"]
"airflow/decorators/__init__.py" = ["D104"]
"airflow/decorators/base.py" = ["D100", "D102", "D105"]
"airflow/decorators/bash.py" = ["D100"]
"airflow/decorators/branch_external_python.py" = ["D100"]
"airflow/decorators/branch_python.py" = ["D100"]
"airflow/decorators/branch_virtualenv.py" = ["D100"]
"airflow/decorators/external_python.py" = ["D100"]
"airflow/decorators/python.py" = ["D100"]
"airflow/decorators/python_virtualenv.py" = ["D100"]
"airflow/decorators/sensor.py" = ["D100"]
"airflow/decorators/setup_teardown.py" = ["D100", "D103", "D105"]
"airflow/decorators/short_circuit.py" = ["D100"]
"airflow/exceptions.py" = ["D105"]
"airflow/executors/base_executor.py" = ["D105"]
"airflow/executors/debug_executor.py" = ["D102"]
"airflow/executors/executor_constants.py" = ["D100"]
"airflow/executors/local_executor.py" = ["D102"]
"airflow/executors/sequential_executor.py" = ["D102"]
"airflow/hooks/base.py" = ["D102"]
"airflow/hooks/filesystem.py" = ["D100", "D102"]
"airflow/hooks/subprocess.py" = ["D100"]
"airflow/io/__init__.py" = ["D104"]
"airflow/io/path.py" = ["D100", "D102", "D105"]
"airflow/io/store/__init__.py" = ["D102", "D104", "D105"]
"airflow/io/typedef.py" = ["D100"]
"airflow/io/utils/__init__.py" = ["D104"]
"airflow/io/utils/stat.py" = ["D100"]
"airflow/jobs/__init__.py" = ["D104"]
"airflow/jobs/backfill_job_runner.py" = ["D100"]
"airflow/jobs/base_job_runner.py" = ["D100"]
"airflow/jobs/dag_processor_job_runner.py" = ["D100", "D103"]
"airflow/jobs/job.py" = ["D100", "D102"]
"airflow/jobs/local_task_job_runner.py" = ["D100", "D102"]
"airflow/jobs/scheduler_job_runner.py" = ["D100", "D102"]
"airflow/jobs/triggerer_job_runner.py" = ["D100", "D102"]
"airflow/kubernetes/__init__.py" = ["D104"]
"airflow/kubernetes/pre_7_4_0_compatibility/__init__.py" = ["D104"]
"airflow/kubernetes/pre_7_4_0_compatibility/secret.py" = ["D105"]
"airflow/listeners/__init__.py" = ["D104"]
"airflow/listeners/listener.py" = ["D100", "D102"]
"airflow/listeners/spec/__init__.py" = ["D104"]
"airflow/listeners/spec/dagrun.py" = ["D100"]
"airflow/listeners/spec/dataset.py" = ["D100"]
"airflow/listeners/spec/lifecycle.py" = ["D100"]
"airflow/listeners/spec/taskinstance.py" = ["D100"]
"airflow/logging_config.py" = ["D100"]
"airflow/macros/__init__.py" = ["D104"]
"airflow/metrics/__init__.py" = ["D104"]
"airflow/metrics/base_stats_logger.py" = ["D100"]
"airflow/metrics/datadog_logger.py" = ["D100"]
"airflow/metrics/otel_logger.py" = ["D100", "D102", "D103"]
"airflow/metrics/protocols.py" = ["D100", "D105"]
"airflow/metrics/statsd_logger.py" = ["D100"]
"airflow/metrics/validators.py" = ["D100", "D102", "D103", "D105"]
"airflow/migrations/__init__.py" = ["D104"]
"airflow/migrations/db_types.py" = ["D100"]
"airflow/migrations/env.py" = ["D100"]
"airflow/migrations/utils.py" = ["D100", "D103"]
"airflow/models/abstractoperator.py" = ["D100", "D102", "D105"]
"airflow/models/base.py" = ["D100", "D103"]
"airflow/models/baseoperator.py" = ["D102", "D103", "D105"]
"airflow/models/baseoperatorlink.py" = ["D100"]
"airflow/models/connection.py" = ["D100", "D102", "D105"]
"airflow/models/crypto.py" = ["D100"]
"airflow/models/dag.py" = ["D100", "D102", "D105"]
"airflow/models/dagbag.py" = ["D100", "D102"]
"airflow/models/dagcode.py" = ["D100"]
"airflow/models/dagpickle.py" = ["D100"]
"airflow/models/dagrun.py" = ["D100", "D102", "D105"]
"airflow/models/dagwarning.py" = ["D100", "D105"]
"airflow/models/dataset.py" = ["D100", "D102", "D105"]
"airflow/models/db_callback_request.py" = ["D100", "D102"]
"airflow/models/errors.py" = ["D100"]
"airflow/models/expandinput.py" = ["D100", "D102", "D103", "D105"]
"airflow/models/log.py" = ["D100", "D105"]
"airflow/models/mappedoperator.py" = ["D100", "D102", "D103", "D105"]
"airflow/models/operator.py" = ["D100"]
"airflow/models/param.py" = ["D100", "D102", "D105"]
"airflow/models/pool.py" = ["D100", "D102", "D105"]
"airflow/models/renderedtifields.py" = ["D105"]
"airflow/models/serialized_dag.py" = ["D102", "D105"]
"airflow/models/skipmixin.py" = ["D100"]
"airflow/models/slamiss.py" = ["D100", "D105"]
"airflow/models/taskfail.py" = ["D105"]
"airflow/models/taskinstance.py" = ["D100", "D102", "D105"]
"airflow/models/taskinstancekey.py" = ["D100"]
"airflow/models/tasklog.py" = ["D100", "D105"]
"airflow/models/taskmap.py" = ["D102"]
"airflow/models/taskmixin.py" = ["D100", "D102", "D105"]
"airflow/models/trigger.py" = ["D100", "D102"]
"airflow/models/variable.py" = ["D100", "D102", "D105"]
"airflow/models/xcom.py" = ["D100", "D102", "D105"]
"airflow/models/xcom_arg.py" = ["D100", "D102", "D105"]
"airflow/notifications/__init__.py" = ["D104"]
"airflow/notifications/basenotifier.py" = ["D100"]
"airflow/operators/bash.py" = ["D100", "D102"]
"airflow/operators/branch.py" = ["D102"]
"airflow/operators/datetime.py" = ["D100", "D102"]
"airflow/operators/email.py" = ["D100", "D102"]
"airflow/operators/empty.py" = ["D100", "D102"]
"airflow/operators/generic_transfer.py" = ["D100", "D102"]
"airflow/operators/latest_only.py" = ["D102"]
"airflow/operators/python.py" = ["D100", "D102"]
"airflow/operators/smooth.py" = ["D100", "D102"]
"airflow/operators/subdag.py" = ["D102"]
"airflow/operators/trigger_dagrun.py" = ["D100", "D102"]
"airflow/operators/weekday.py" = ["D100", "D102"]
"airflow/plugins_manager.py" = ["D103", "D105"]
"airflow/policies.py" = ["D100"]
"airflow/providers/airbyte/__init__.py" = ["D104"]
"airflow/providers/airbyte/hooks/__init__.py" = ["D104"]
"airflow/providers/airbyte/hooks/airbyte.py" = ["D100"]
"airflow/providers/airbyte/operators/__init__.py" = ["D104"]
"airflow/providers/airbyte/operators/airbyte.py" = ["D100"]
"airflow/providers/airbyte/sensors/__init__.py" = ["D104"]
"airflow/providers/airbyte/sensors/airbyte.py" = ["D102"]
"airflow/providers/airbyte/triggers/__init__.py" = ["D104"]
"airflow/providers/airbyte/triggers/airbyte.py" = ["D100"]
"airflow/providers/alibaba/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/hooks/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/hooks/analyticdb_spark.py" = ["D100"]
"airflow/providers/alibaba/cloud/hooks/oss.py" = ["D100", "D102"]
"airflow/providers/alibaba/cloud/log/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/log/oss_task_handler.py" = ["D100", "D102"]
"airflow/providers/alibaba/cloud/operators/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/operators/analyticdb_spark.py" = ["D100", "D102"]
"airflow/providers/alibaba/cloud/operators/oss.py" = ["D102"]
"airflow/providers/alibaba/cloud/sensors/__init__.py" = ["D104"]
"airflow/providers/alibaba/cloud/sensors/analyticdb_spark.py" = ["D100", "D102"]
"airflow/providers/alibaba/cloud/sensors/oss_key.py" = ["D100"]
"airflow/providers/amazon/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/avp/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/avp/entities.py" = ["D100"]
"airflow/providers/amazon/aws/auth_manager/avp/facade.py" = ["D100"]
"airflow/providers/amazon/aws/auth_manager/aws_auth_manager.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/auth_manager/cli/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/cli/definition.py" = ["D100"]
"airflow/providers/amazon/aws/auth_manager/constants.py" = ["D100"]
"airflow/providers/amazon/aws/auth_manager/security_manager/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/security_manager/aws_security_manager_override.py" = ["D100"]
"airflow/providers/amazon/aws/auth_manager/user.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/auth_manager/views/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/auth_manager/views/auth.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/exceptions.py" = ["D100", "D105"]
"airflow/providers/amazon/aws/executors/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/executors/ecs/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/executors/ecs/ecs_executor.py" = ["D102"]
"airflow/providers/amazon/aws/executors/ecs/ecs_executor_config.py" = ["D103"]
"airflow/providers/amazon/aws/executors/ecs/utils.py" = ["D105"]
"airflow/providers/amazon/aws/executors/utils/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/executors/utils/exponential_backoff_retry.py" = ["D100"]
"airflow/providers/amazon/aws/fs/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/fs/s3.py" = ["D100", "D103"]
"airflow/providers/amazon/aws/hooks/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/hooks/appflow.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/athena.py" = ["D103"]
"airflow/providers/amazon/aws/hooks/athena_sql.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/base_aws.py" = ["D102"]
"airflow/providers/amazon/aws/hooks/batch_client.py" = ["D102"]
"airflow/providers/amazon/aws/hooks/chime.py" = ["D102"]
"airflow/providers/amazon/aws/hooks/dms.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/ec2.py" = ["D100", "D102", "D103"]
"airflow/providers/amazon/aws/hooks/ecr.py" = ["D100", "D105"]
"airflow/providers/amazon/aws/hooks/ecs.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/eks.py" = ["D102"]
"airflow/providers/amazon/aws/hooks/elasticache_replication_group.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/emr.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/eventbridge.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/glacier.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/glue.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/glue_crawler.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/glue_databrew.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/logs.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/neptune.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/quicksight.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/redshift_cluster.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/redshift_data.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/redshift_sql.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/sagemaker.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/hooks/secrets_manager.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/ssm.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/step_function.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/sts.py" = ["D100"]
"airflow/providers/amazon/aws/hooks/verified_permissions.py" = ["D100"]
"airflow/providers/amazon/aws/links/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/links/athena.py" = ["D100"]
"airflow/providers/amazon/aws/links/base_aws.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/links/batch.py" = ["D100"]
"airflow/providers/amazon/aws/links/emr.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/links/glue.py" = ["D100"]
"airflow/providers/amazon/aws/links/logs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/links/step_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/log/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/log/cloudwatch_task_handler.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/log/s3_task_handler.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/notifications/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/notifications/chime.py" = ["D100"]
"airflow/providers/amazon/aws/notifications/sns.py" = ["D100"]
"airflow/providers/amazon/aws/notifications/sqs.py" = ["D100"]
"airflow/providers/amazon/aws/operators/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/operators/appflow.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/athena.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/base_aws.py" = ["D100"]
"airflow/providers/amazon/aws/operators/batch.py" = ["D102"]
"airflow/providers/amazon/aws/operators/cloud_formation.py" = ["D102"]
"airflow/providers/amazon/aws/operators/datasync.py" = ["D102"]
"airflow/providers/amazon/aws/operators/dms.py" = ["D100"]
"airflow/providers/amazon/aws/operators/ec2.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/ecs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/eks.py" = ["D102"]
"airflow/providers/amazon/aws/operators/emr.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/eventbridge.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/glacier.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/glue.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/glue_crawler.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/glue_databrew.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/lambda_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/neptune.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/quicksight.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/rds.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/redshift_cluster.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/redshift_data.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/operators/s3.py" = ["D102"]
"airflow/providers/amazon/aws/operators/sagemaker.py" = ["D100", "D102", "D103"]
"airflow/providers/amazon/aws/operators/sns.py" = ["D102"]
"airflow/providers/amazon/aws/operators/step_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/secrets/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/sensors/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/sensors/athena.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/base_aws.py" = ["D100"]
"airflow/providers/amazon/aws/sensors/batch.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/cloud_formation.py" = ["D102"]
"airflow/providers/amazon/aws/sensors/dms.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/dynamodb.py" = ["D100"]
"airflow/providers/amazon/aws/sensors/ec2.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/ecs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/eks.py" = ["D102"]
"airflow/providers/amazon/aws/sensors/emr.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/glacier.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/glue.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/glue_catalog_partition.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/glue_crawler.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/lambda_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/quicksight.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/rds.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/redshift_cluster.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/sagemaker.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/sensors/sqs.py" = ["D102"]
"airflow/providers/amazon/aws/sensors/step_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/transfers/azure_blob_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/dynamodb_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/exasol_to_s3.py" = ["D102"]
"airflow/providers/amazon/aws/transfers/ftp_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/gcs_to_s3.py" = ["D102"]
"airflow/providers/amazon/aws/transfers/glacier_to_gcs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/hive_to_dynamodb.py" = ["D102"]
"airflow/providers/amazon/aws/transfers/http_to_s3.py" = ["D102"]
"airflow/providers/amazon/aws/transfers/local_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/mongo_to_s3.py" = ["D100"]
"airflow/providers/amazon/aws/transfers/redshift_to_s3.py" = ["D102"]
"airflow/providers/amazon/aws/transfers/s3_to_ftp.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/s3_to_redshift.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/s3_to_sftp.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/s3_to_sql.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/salesforce_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/sftp_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/transfers/sql_to_s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/triggers/athena.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/base.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/batch.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/ec2.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/ecs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/eks.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/emr.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/glue.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/glue_crawler.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/glue_databrew.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/lambda_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/neptune.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/rds.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/redshift_cluster.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/redshift_data.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/s3.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/sagemaker.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/sqs.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/triggers/step_function.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/utils/__init__.py" = ["D103", "D104"]
"airflow/providers/amazon/aws/utils/connection_wrapper.py" = ["D100", "D102", "D105"]
"airflow/providers/amazon/aws/utils/eks_get_token.py" = ["D100", "D103"]
"airflow/providers/amazon/aws/utils/identifiers.py" = ["D100"]
"airflow/providers/amazon/aws/utils/rds.py" = ["D100"]
"airflow/providers/amazon/aws/utils/redshift.py" = ["D100"]
"airflow/providers/amazon/aws/utils/sagemaker.py" = ["D100"]
"airflow/providers/amazon/aws/utils/sqs.py" = ["D100", "D103"]
"airflow/providers/amazon/aws/utils/tags.py" = ["D100"]
"airflow/providers/amazon/aws/utils/task_log_fetcher.py" = ["D100", "D102"]
"airflow/providers/amazon/aws/utils/waiter.py" = ["D100", "D103"]
"airflow/providers/amazon/aws/utils/waiter_with_logging.py" = ["D100"]
"airflow/providers/amazon/aws/waiters/__init__.py" = ["D104"]
"airflow/providers/amazon/aws/waiters/base_waiter.py" = ["D100", "D102"]
"airflow/providers/apache/__init__.py" = ["D104"]
"airflow/providers/apache/beam/__init__.py" = ["D104"]
"airflow/providers/apache/beam/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/beam/hooks/beam.py" = ["D102"]
"airflow/providers/apache/beam/operators/__init__.py" = ["D104"]
"airflow/providers/apache/beam/operators/beam.py" = ["D102"]
"airflow/providers/apache/beam/triggers/__init__.py" = ["D104"]
"airflow/providers/apache/beam/triggers/beam.py" = ["D100"]
"airflow/providers/apache/cassandra/__init__.py" = ["D104"]
"airflow/providers/apache/cassandra/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/cassandra/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/cassandra/sensors/record.py" = ["D100", "D102"]
"airflow/providers/apache/cassandra/sensors/table.py" = ["D100", "D102"]
"airflow/providers/apache/drill/__init__.py" = ["D104"]
"airflow/providers/apache/drill/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/drill/hooks/drill.py" = ["D100", "D102"]
"airflow/providers/apache/drill/operators/__init__.py" = ["D104"]
"airflow/providers/apache/drill/operators/drill.py" = ["D100"]
"airflow/providers/apache/druid/__init__.py" = ["D104"]
"airflow/providers/apache/druid/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/druid/hooks/druid.py" = ["D100", "D102"]
"airflow/providers/apache/druid/operators/__init__.py" = ["D104"]
"airflow/providers/apache/druid/operators/druid.py" = ["D100", "D102"]
"airflow/providers/apache/druid/operators/druid_check.py" = ["D100"]
"airflow/providers/apache/druid/transfers/__init__.py" = ["D104"]
"airflow/providers/apache/druid/transfers/hive_to_druid.py" = ["D102"]
"airflow/providers/apache/flink/__init__.py" = ["D104"]
"airflow/providers/apache/flink/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/flink/operators/__init__.py" = ["D104"]
"airflow/providers/apache/flink/operators/flink_kubernetes.py" = ["D100", "D102"]
"airflow/providers/apache/flink/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/flink/sensors/flink_kubernetes.py" = ["D100", "D102"]
"airflow/providers/apache/hdfs/__init__.py" = ["D104"]
"airflow/providers/apache/hdfs/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/hdfs/hooks/hdfs.py" = ["D100"]
"airflow/providers/apache/hdfs/log/__init__.py" = ["D104"]
"airflow/providers/apache/hdfs/log/hdfs_task_handler.py" = ["D100", "D102"]
"airflow/providers/apache/hdfs/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/hdfs/sensors/hdfs.py" = ["D100"]
"airflow/providers/apache/hdfs/sensors/web_hdfs.py" = ["D100", "D102"]
"airflow/providers/apache/hive/__init__.py" = ["D104"]
"airflow/providers/apache/hive/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/hive/hooks/hive.py" = ["D100", "D102", "D105"]
"airflow/providers/apache/hive/macros/__init__.py" = ["D104"]
"airflow/providers/apache/hive/macros/hive.py" = ["D100"]
"airflow/providers/apache/hive/operators/__init__.py" = ["D104"]
"airflow/providers/apache/hive/operators/hive.py" = ["D100", "D102"]
"airflow/providers/apache/hive/operators/hive_stats.py" = ["D100", "D102"]
"airflow/providers/apache/hive/plugins/__init__.py" = ["D104"]
"airflow/providers/apache/hive/plugins/hive.py" = ["D100"]
"airflow/providers/apache/hive/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/hive/sensors/hive_partition.py" = ["D100", "D102"]
"airflow/providers/apache/hive/sensors/metastore_partition.py" = ["D100", "D102"]
"airflow/providers/apache/hive/sensors/named_hive_partition.py" = ["D100", "D102"]
"airflow/providers/apache/hive/transfers/__init__.py" = ["D104"]
"airflow/providers/apache/hive/transfers/hive_to_mysql.py" = ["D102"]
"airflow/providers/apache/hive/transfers/hive_to_samba.py" = ["D102"]
"airflow/providers/apache/hive/transfers/mssql_to_hive.py" = ["D102"]
"airflow/providers/apache/hive/transfers/mysql_to_hive.py" = ["D102"]
"airflow/providers/apache/hive/transfers/s3_to_hive.py" = ["D102"]
"airflow/providers/apache/hive/transfers/vertica_to_hive.py" = ["D102"]
"airflow/providers/apache/impala/__init__.py" = ["D104"]
"airflow/providers/apache/impala/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/impala/hooks/impala.py" = ["D100", "D102"]
"airflow/providers/apache/kafka/__init__.py" = ["D104"]
"airflow/providers/apache/kafka/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/kafka/hooks/base.py" = ["D100"]
"airflow/providers/apache/kafka/hooks/client.py" = ["D100"]
"airflow/providers/apache/kafka/hooks/consume.py" = ["D100"]
"airflow/providers/apache/kafka/hooks/produce.py" = ["D100"]
"airflow/providers/apache/kafka/operators/__init__.py" = ["D104"]
"airflow/providers/apache/kafka/operators/consume.py" = ["D100", "D102"]
"airflow/providers/apache/kafka/operators/produce.py" = ["D100", "D102", "D103"]
"airflow/providers/apache/kafka/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/kafka/sensors/kafka.py" = ["D100", "D102"]
"airflow/providers/apache/kafka/triggers/__init__.py" = ["D104"]
"airflow/providers/apache/kafka/triggers/await_message.py" = ["D100", "D102"]
"airflow/providers/apache/kylin/__init__.py" = ["D104"]
"airflow/providers/apache/kylin/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/kylin/hooks/kylin.py" = ["D100", "D102"]
"airflow/providers/apache/kylin/operators/__init__.py" = ["D104"]
"airflow/providers/apache/kylin/operators/kylin_cube.py" = ["D100", "D102"]
"airflow/providers/apache/livy/__init__.py" = ["D104"]
"airflow/providers/apache/livy/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/livy/operators/__init__.py" = ["D104"]
"airflow/providers/apache/livy/operators/livy.py" = ["D102"]
"airflow/providers/apache/livy/sensors/__init__.py" = ["D104"]
"airflow/providers/apache/livy/sensors/livy.py" = ["D102"]
"airflow/providers/apache/livy/triggers/__init__.py" = ["D104"]
"airflow/providers/apache/pig/__init__.py" = ["D104"]
"airflow/providers/apache/pig/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/pig/hooks/pig.py" = ["D100"]
"airflow/providers/apache/pig/operators/__init__.py" = ["D104"]
"airflow/providers/apache/pig/operators/pig.py" = ["D100", "D102"]
"airflow/providers/apache/pinot/__init__.py" = ["D104"]
"airflow/providers/apache/pinot/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/pinot/hooks/pinot.py" = ["D100", "D102"]
"airflow/providers/apache/spark/__init__.py" = ["D104"]
"airflow/providers/apache/spark/decorators/__init__.py" = ["D104"]
"airflow/providers/apache/spark/decorators/pyspark.py" = ["D100", "D103"]
"airflow/providers/apache/spark/hooks/__init__.py" = ["D104"]
"airflow/providers/apache/spark/hooks/spark_connect.py" = ["D100", "D102"]
"airflow/providers/apache/spark/hooks/spark_jdbc.py" = ["D100", "D102"]
"airflow/providers/apache/spark/hooks/spark_jdbc_script.py" = ["D100"]
"airflow/providers/apache/spark/hooks/spark_sql.py" = ["D100", "D102"]
"airflow/providers/apache/spark/hooks/spark_submit.py" = ["D100", "D102"]
"airflow/providers/apache/spark/operators/__init__.py" = ["D104"]
"airflow/providers/apache/spark/operators/spark_jdbc.py" = ["D100", "D102"]
"airflow/providers/apache/spark/operators/spark_sql.py" = ["D100", "D102"]
"airflow/providers/apache/spark/operators/spark_submit.py" = ["D100", "D102"]
"airflow/providers/apprise/__init__.py" = ["D104"]
"airflow/providers/apprise/hooks/__init__.py" = ["D104"]
"airflow/providers/apprise/hooks/apprise.py" = ["D100", "D102"]
"airflow/providers/apprise/notifications/__init__.py" = ["D104"]
"airflow/providers/apprise/notifications/apprise.py" = ["D100"]
"airflow/providers/arangodb/__init__.py" = ["D104"]
"airflow/providers/arangodb/hooks/__init__.py" = ["D104"]
"airflow/providers/arangodb/hooks/arangodb.py" = ["D102"]
"airflow/providers/arangodb/operators/__init__.py" = ["D104"]
"airflow/providers/arangodb/operators/arangodb.py" = ["D100", "D102"]
"airflow/providers/arangodb/sensors/__init__.py" = ["D104"]
"airflow/providers/arangodb/sensors/arangodb.py" = ["D100", "D102"]
"airflow/providers/asana/__init__.py" = ["D104"]
"airflow/providers/asana/hooks/__init__.py" = ["D104"]
"airflow/providers/asana/hooks/asana.py" = ["D102"]
"airflow/providers/asana/operators/__init__.py" = ["D104"]
"airflow/providers/asana/operators/asana_tasks.py" = ["D100", "D102"]
"airflow/providers/atlassian/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/hooks/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/hooks/jira.py" = ["D102"]
"airflow/providers/atlassian/jira/notifications/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/notifications/jira.py" = ["D100", "D102"]
"airflow/providers/atlassian/jira/operators/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/operators/jira.py" = ["D100", "D102"]
"airflow/providers/atlassian/jira/sensors/__init__.py" = ["D104"]
"airflow/providers/atlassian/jira/sensors/jira.py" = ["D100", "D102"]
"airflow/providers/celery/__init__.py" = ["D104"]
"airflow/providers/celery/executors/__init__.py" = ["D104"]
"airflow/providers/celery/executors/celery_executor.py" = ["D102"]
"airflow/providers/celery/executors/celery_kubernetes_executor.py" = ["D100", "D102"]
"airflow/providers/celery/sensors/__init__.py" = ["D104"]
"airflow/providers/celery/sensors/celery_queue.py" = ["D100", "D102"]
"airflow/providers/cloudant/__init__.py" = ["D104"]
"airflow/providers/cloudant/hooks/__init__.py" = ["D104"]
"airflow/providers/cncf/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/backcompat/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/callbacks.py" = ["D100"]
"airflow/providers/cncf/kubernetes/decorators/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/decorators/kubernetes.py" = ["D100"]
"airflow/providers/cncf/kubernetes/executors/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py" = ["D102"]
"airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py" = ["D100"]
"airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/executors/local_kubernetes_executor.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/hooks/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/hooks/kubernetes.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/kube_config.py" = ["D100"]
"airflow/providers/cncf/kubernetes/kubernetes_executor_templates/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/kubernetes_helper_functions.py" = ["D100", "D103"]
"airflow/providers/cncf/kubernetes/operators/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/operators/custom_object_launcher.py" = ["D102"]
"airflow/providers/cncf/kubernetes/operators/job.py" = ["D102"]
"airflow/providers/cncf/kubernetes/operators/pod.py" = ["D102"]
"airflow/providers/cncf/kubernetes/operators/resource.py" = ["D102"]
"airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/pod_template_file_examples/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/resource_convert/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/resource_convert/configmap.py" = ["D100"]
"airflow/providers/cncf/kubernetes/resource_convert/env_variable.py" = ["D100"]
"airflow/providers/cncf/kubernetes/resource_convert/secret.py" = ["D100"]
"airflow/providers/cncf/kubernetes/secret.py" = ["D105"]
"airflow/providers/cncf/kubernetes/sensors/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/sensors/spark_kubernetes.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/template_rendering.py" = ["D100"]
"airflow/providers/cncf/kubernetes/triggers/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/triggers/pod.py" = ["D100", "D102"]
"airflow/providers/cncf/kubernetes/utils/__init__.py" = ["D104"]
"airflow/providers/cncf/kubernetes/utils/delete_from.py" = ["D100", "D103", "D105"]
"airflow/providers/cncf/kubernetes/utils/k8s_resource_iterator.py" = ["D100", "D103"]
"airflow/providers/cncf/kubernetes/utils/pod_manager.py" = ["D102", "D103"]
"airflow/providers/cohere/__init__.py" = ["D104"]
"airflow/providers/cohere/hooks/__init__.py" = ["D104"]
"airflow/providers/cohere/hooks/cohere.py" = ["D100", "D102"]
"airflow/providers/cohere/operators/__init__.py" = ["D104"]
"airflow/providers/cohere/operators/embedding.py" = ["D100"]
"airflow/providers/common/__init__.py" = ["D104"]
"airflow/providers/common/io/__init__.py" = ["D104"]
"airflow/providers/common/io/operators/__init__.py" = ["D104"]
"airflow/providers/common/io/operators/file_transfer.py" = ["D100", "D102"]
"airflow/providers/common/io/xcom/__init__.py" = ["D104"]
"airflow/providers/common/io/xcom/backend.py" = ["D100", "D102"]
"airflow/providers/common/sql/__init__.py" = ["D104"]
"airflow/providers/common/sql/hooks/__init__.py" = ["D104"]
"airflow/providers/common/sql/hooks/sql.py" = ["D100", "D102"]
"airflow/providers/common/sql/operators/__init__.py" = ["D104"]
"airflow/providers/common/sql/operators/sql.py" = ["D100", "D102"]
"airflow/providers/common/sql/sensors/__init__.py" = ["D104"]
"airflow/providers/common/sql/sensors/sql.py" = ["D100", "D102"]
"airflow/providers/databricks/__init__.py" = ["D104"]
"airflow/providers/databricks/hooks/__init__.py" = ["D104"]
"airflow/providers/databricks/hooks/databricks.py" = ["D102", "D105"]
"airflow/providers/databricks/hooks/databricks_base.py" = ["D102", "D105"]
"airflow/providers/databricks/hooks/databricks_sql.py" = ["D100", "D102"]
"airflow/providers/databricks/operators/__init__.py" = ["D104"]
"airflow/providers/databricks/operators/databricks.py" = ["D102"]
"airflow/providers/databricks/operators/databricks_repos.py" = ["D102", "D105"]
"airflow/providers/databricks/operators/databricks_sql.py" = ["D102"]
"airflow/providers/databricks/sensors/__init__.py" = ["D104"]
"airflow/providers/databricks/triggers/__init__.py" = ["D104"]
"airflow/providers/databricks/triggers/databricks.py" = ["D100", "D102"]
"airflow/providers/databricks/utils/__init__.py" = ["D104"]
"airflow/providers/databricks/utils/databricks.py" = ["D100"]
"airflow/providers/datadog/__init__.py" = ["D104"]
"airflow/providers/datadog/hooks/__init__.py" = ["D104"]
"airflow/providers/datadog/hooks/datadog.py" = ["D100"]
"airflow/providers/datadog/sensors/__init__.py" = ["D104"]
"airflow/providers/datadog/sensors/datadog.py" = ["D100", "D102"]
"airflow/providers/dbt/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/hooks/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/hooks/dbt.py" = ["D100", "D102"]
"airflow/providers/dbt/cloud/operators/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/operators/dbt.py" = ["D100", "D102"]
"airflow/providers/dbt/cloud/sensors/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/sensors/dbt.py" = ["D100", "D102"]
"airflow/providers/dbt/cloud/triggers/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/triggers/dbt.py" = ["D100"]
"airflow/providers/dbt/cloud/utils/__init__.py" = ["D104"]
"airflow/providers/dbt/cloud/utils/openlineage.py" = ["D100"]
"airflow/providers/dingding/__init__.py" = ["D104"]
"airflow/providers/dingding/hooks/__init__.py" = ["D104"]
"airflow/providers/dingding/hooks/dingding.py" = ["D100"]
"airflow/providers/dingding/operators/__init__.py" = ["D104"]
"airflow/providers/dingding/operators/dingding.py" = ["D100", "D102"]
"airflow/providers/discord/__init__.py" = ["D104"]
"airflow/providers/discord/hooks/__init__.py" = ["D104"]
"airflow/providers/discord/hooks/discord_webhook.py" = ["D100"]
"airflow/providers/discord/notifications/__init__.py" = ["D104"]
"airflow/providers/discord/notifications/discord.py" = ["D100"]
"airflow/providers/discord/operators/__init__.py" = ["D104"]
"airflow/providers/discord/operators/discord_webhook.py" = ["D100", "D102"]
"airflow/providers/docker/__init__.py" = ["D104"]
"airflow/providers/docker/decorators/__init__.py" = ["D104"]
"airflow/providers/docker/decorators/docker.py" = ["D100"]
"airflow/providers/docker/hooks/__init__.py" = ["D104"]
"airflow/providers/docker/hooks/docker.py" = ["D100"]
"airflow/providers/docker/operators/__init__.py" = ["D104"]
"airflow/providers/docker/operators/docker.py" = ["D102"]
"airflow/providers/docker/operators/docker_swarm.py" = ["D102"]
"airflow/providers/elasticsearch/__init__.py" = ["D104"]
"airflow/providers/elasticsearch/hooks/__init__.py" = ["D104"]
"airflow/providers/elasticsearch/hooks/elasticsearch.py" = ["D100", "D102", "D103"]
"airflow/providers/elasticsearch/log/__init__.py" = ["D104"]
"airflow/providers/elasticsearch/log/es_json_formatter.py" = ["D100"]
"airflow/providers/elasticsearch/log/es_response.py" = ["D100", "D102", "D105"]
"airflow/providers/elasticsearch/log/es_task_handler.py" = ["D100", "D102", "D103"]
"airflow/providers/exasol/__init__.py" = ["D104"]
"airflow/providers/exasol/hooks/__init__.py" = ["D104"]
"airflow/providers/exasol/hooks/exasol.py" = ["D100", "D102", "D103"]
"airflow/providers/exasol/operators/__init__.py" = ["D104"]
"airflow/providers/exasol/operators/exasol.py" = ["D100"]
"airflow/providers/fab/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/api/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/api/auth/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/api/auth/backend/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/api/auth/backend/kerberos_auth.py" = ["D100", "D103"]
"airflow/providers/fab/auth_manager/api_endpoints/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/api_endpoints/role_and_permission_endpoint.py" = ["D100"]
"airflow/providers/fab/auth_manager/api_endpoints/user_endpoint.py" = ["D100"]
"airflow/providers/fab/auth_manager/cli_commands/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/cli_commands/definition.py" = ["D100"]
"airflow/providers/fab/auth_manager/cli_commands/utils.py" = ["D100", "D103"]
"airflow/providers/fab/auth_manager/decorators/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/decorators/auth.py" = ["D100"]
"airflow/providers/fab/auth_manager/fab_auth_manager.py" = ["D100", "D102"]
"airflow/providers/fab/auth_manager/models/__init__.py" = ["D102", "D103", "D104", "D105"]
"airflow/providers/fab/auth_manager/models/anonymous_user.py" = ["D100", "D102"]
"airflow/providers/fab/auth_manager/openapi/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/security_manager/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/security_manager/constants.py" = ["D100"]
"airflow/providers/fab/auth_manager/security_manager/override.py" = ["D100", "D102"]
"airflow/providers/fab/auth_manager/views/__init__.py" = ["D104"]
"airflow/providers/fab/auth_manager/views/permissions.py" = ["D100"]
"airflow/providers/fab/auth_manager/views/roles_list.py" = ["D100"]
"airflow/providers/fab/auth_manager/views/user.py" = ["D100", "D102"]
"airflow/providers/fab/auth_manager/views/user_edit.py" = ["D100"]
"airflow/providers/fab/auth_manager/views/user_stats.py" = ["D100"]
"airflow/providers/facebook/__init__.py" = ["D104"]
"airflow/providers/facebook/ads/__init__.py" = ["D104"]
"airflow/providers/facebook/ads/hooks/__init__.py" = ["D104"]
"airflow/providers/ftp/__init__.py" = ["D104"]
"airflow/providers/ftp/hooks/__init__.py" = ["D104"]
"airflow/providers/ftp/hooks/ftp.py" = ["D100", "D105"]
"airflow/providers/ftp/operators/__init__.py" = ["D104"]
"airflow/providers/ftp/operators/ftp.py" = ["D102"]
"airflow/providers/ftp/sensors/__init__.py" = ["D104"]
"airflow/providers/ftp/sensors/ftp.py" = ["D100", "D102"]
"airflow/providers/github/__init__.py" = ["D104"]
"airflow/providers/github/hooks/__init__.py" = ["D104"]
"airflow/providers/github/operators/__init__.py" = ["D104"]
"airflow/providers/github/operators/github.py" = ["D100", "D102"]
"airflow/providers/github/sensors/__init__.py" = ["D104"]
"airflow/providers/github/sensors/github.py" = ["D100", "D102"]
"airflow/providers/google/__init__.py" = ["D104"]
"airflow/providers/google/ads/__init__.py" = ["D104"]
"airflow/providers/google/ads/hooks/__init__.py" = ["D104"]
"airflow/providers/google/ads/operators/__init__.py" = ["D104"]
"airflow/providers/google/ads/operators/ads.py" = ["D102"]
"airflow/providers/google/ads/transfers/__init__.py" = ["D104"]
"airflow/providers/google/ads/transfers/ads_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/__init__.py" = ["D104"]
"airflow/providers/google/cloud/fs/__init__.py" = ["D104"]
"airflow/providers/google/cloud/fs/gcs.py" = ["D100", "D103"]
"airflow/providers/google/cloud/hooks/__init__.py" = ["D104"]
"airflow/providers/google/cloud/hooks/bigquery.py" = ["D102", "D103"]
"airflow/providers/google/cloud/hooks/cloud_batch.py" = ["D100", "D102"]
"airflow/providers/google/cloud/hooks/cloud_composer.py" = ["D100", "D102"]
"airflow/providers/google/cloud/hooks/cloud_run.py" = ["D100", "D102"]
"airflow/providers/google/cloud/hooks/cloud_sql.py" = ["D102"]
"airflow/providers/google/cloud/hooks/compute_ssh.py" = ["D100", "D102"]
"airflow/providers/google/cloud/hooks/datacatalog.py" = ["D100"]
"airflow/providers/google/cloud/hooks/dataflow.py" = ["D102"]
"airflow/providers/google/cloud/hooks/dataform.py" = ["D100"]
"airflow/providers/google/cloud/hooks/datafusion.py" = ["D102"]
"airflow/providers/google/cloud/hooks/datapipeline.py" = ["D102"]
"airflow/providers/google/cloud/hooks/dataproc.py" = ["D102"]
"airflow/providers/google/cloud/hooks/gdm.py" = ["D100"]
"airflow/providers/google/cloud/hooks/kubernetes_engine.py" = ["D102"]
"airflow/providers/google/cloud/hooks/vertex_ai/__init__.py" = ["D104"]
"airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py" = ["D102"]
"airflow/providers/google/cloud/hooks/workflows.py" = ["D100"]
"airflow/providers/google/cloud/links/__init__.py" = ["D104"]
"airflow/providers/google/cloud/links/automl.py" = ["D102"]
"airflow/providers/google/cloud/links/base.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/bigquery.py" = ["D102"]
"airflow/providers/google/cloud/links/bigquery_dts.py" = ["D102"]
"airflow/providers/google/cloud/links/bigtable.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/cloud_build.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/cloud_functions.py" = ["D102"]
"airflow/providers/google/cloud/links/cloud_memorystore.py" = ["D102"]
"airflow/providers/google/cloud/links/cloud_sql.py" = ["D102"]
"airflow/providers/google/cloud/links/cloud_storage_transfer.py" = ["D102"]
"airflow/providers/google/cloud/links/cloud_tasks.py" = ["D102"]
"airflow/providers/google/cloud/links/compute.py" = ["D102"]
"airflow/providers/google/cloud/links/data_loss_prevention.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/datacatalog.py" = ["D102"]
"airflow/providers/google/cloud/links/dataflow.py" = ["D102"]
"airflow/providers/google/cloud/links/dataform.py" = ["D102"]
"airflow/providers/google/cloud/links/datafusion.py" = ["D102"]
"airflow/providers/google/cloud/links/dataplex.py" = ["D102"]
"airflow/providers/google/cloud/links/dataprep.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/dataproc.py" = ["D102", "D105"]
"airflow/providers/google/cloud/links/datastore.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/kubernetes_engine.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/life_sciences.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/mlengine.py" = ["D102"]
"airflow/providers/google/cloud/links/pubsub.py" = ["D102"]
"airflow/providers/google/cloud/links/spanner.py" = ["D102"]
"airflow/providers/google/cloud/links/stackdriver.py" = ["D102"]
"airflow/providers/google/cloud/links/vertex_ai.py" = ["D100", "D102"]
"airflow/providers/google/cloud/links/workflows.py" = ["D102"]
"airflow/providers/google/cloud/log/__init__.py" = ["D104"]
"airflow/providers/google/cloud/log/gcs_task_handler.py" = ["D100", "D102"]
"airflow/providers/google/cloud/log/stackdriver_task_handler.py" = ["D102"]
"airflow/providers/google/cloud/operators/__init__.py" = ["D104"]
"airflow/providers/google/cloud/operators/automl.py" = ["D102"]
"airflow/providers/google/cloud/operators/bigquery.py" = ["D102"]
"airflow/providers/google/cloud/operators/bigquery_dts.py" = ["D102"]
"airflow/providers/google/cloud/operators/bigtable.py" = ["D102"]
"airflow/providers/google/cloud/operators/cloud_batch.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/cloud_build.py" = ["D102"]
"airflow/providers/google/cloud/operators/cloud_composer.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/cloud_memorystore.py" = ["D102"]
"airflow/providers/google/cloud/operators/cloud_run.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/cloud_sql.py" = ["D102"]
"airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py" = ["D102"]
"airflow/providers/google/cloud/operators/compute.py" = ["D102"]
"airflow/providers/google/cloud/operators/datacatalog.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/dataflow.py" = ["D102"]
"airflow/providers/google/cloud/operators/dataform.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/datafusion.py" = ["D102"]
"airflow/providers/google/cloud/operators/datapipeline.py" = ["D102"]
"airflow/providers/google/cloud/operators/dataplex.py" = ["D102"]
"airflow/providers/google/cloud/operators/dataprep.py" = ["D102"]
"airflow/providers/google/cloud/operators/dataproc.py" = ["D102"]
"airflow/providers/google/cloud/operators/dataproc_metastore.py" = ["D102"]
"airflow/providers/google/cloud/operators/datastore.py" = ["D102"]
"airflow/providers/google/cloud/operators/dlp.py" = ["D102"]
"airflow/providers/google/cloud/operators/functions.py" = ["D102"]
"airflow/providers/google/cloud/operators/gcs.py" = ["D102"]
"airflow/providers/google/cloud/operators/kubernetes_engine.py" = ["D102"]
"airflow/providers/google/cloud/operators/life_sciences.py" = ["D102"]
"airflow/providers/google/cloud/operators/looker.py" = ["D102"]
"airflow/providers/google/cloud/operators/mlengine.py" = ["D102"]
"airflow/providers/google/cloud/operators/natural_language.py" = ["D102"]
"airflow/providers/google/cloud/operators/pubsub.py" = ["D102"]
"airflow/providers/google/cloud/operators/spanner.py" = ["D102"]
"airflow/providers/google/cloud/operators/speech_to_text.py" = ["D102"]
"airflow/providers/google/cloud/operators/stackdriver.py" = ["D100", "D102"]
"airflow/providers/google/cloud/operators/tasks.py" = ["D102"]
"airflow/providers/google/cloud/operators/text_to_speech.py" = ["D102"]
"airflow/providers/google/cloud/operators/translate.py" = ["D102"]
"airflow/providers/google/cloud/operators/translate_speech.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/__init__.py" = ["D104"]
"airflow/providers/google/cloud/operators/vertex_ai/auto_ml.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/custom_job.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/dataset.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/endpoint_service.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/model_service.py" = ["D102"]
"airflow/providers/google/cloud/operators/vertex_ai/pipeline_job.py" = ["D102"]
"airflow/providers/google/cloud/operators/video_intelligence.py" = ["D102"]
"airflow/providers/google/cloud/operators/vision.py" = ["D102"]
"airflow/providers/google/cloud/operators/workflows.py" = ["D100", "D102"]
"airflow/providers/google/cloud/secrets/__init__.py" = ["D104"]
"airflow/providers/google/cloud/sensors/__init__.py" = ["D104"]
"airflow/providers/google/cloud/sensors/bigquery.py" = ["D102"]
"airflow/providers/google/cloud/sensors/bigquery_dts.py" = ["D102"]
"airflow/providers/google/cloud/sensors/bigtable.py" = ["D102"]
"airflow/providers/google/cloud/sensors/cloud_storage_transfer_service.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataflow.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataform.py" = ["D102"]
"airflow/providers/google/cloud/sensors/datafusion.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataplex.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataprep.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataproc.py" = ["D102"]
"airflow/providers/google/cloud/sensors/dataproc_metastore.py" = ["D100", "D102"]
"airflow/providers/google/cloud/sensors/gcs.py" = ["D102"]
"airflow/providers/google/cloud/sensors/looker.py" = ["D102"]
"airflow/providers/google/cloud/sensors/pubsub.py" = ["D102"]
"airflow/providers/google/cloud/sensors/tasks.py" = ["D102"]
"airflow/providers/google/cloud/sensors/workflows.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/__init__.py" = ["D104"]
"airflow/providers/google/cloud/transfers/adls_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/azure_blob_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/azure_fileshare_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_bigquery.py" = ["D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_mssql.py" = ["D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_mysql.py" = ["D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_postgres.py" = ["D102"]
"airflow/providers/google/cloud/transfers/bigquery_to_sql.py" = ["D102"]
"airflow/providers/google/cloud/transfers/calendar_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/cassandra_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/facebook_ads_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/gcs_to_bigquery.py" = ["D102"]
"airflow/providers/google/cloud/transfers/gcs_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/gcs_to_local.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/gcs_to_sftp.py" = ["D102"]
"airflow/providers/google/cloud/transfers/gdrive_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/gdrive_to_local.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/mssql_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/mysql_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/oracle_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/postgres_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/presto_to_gcs.py" = ["D100"]
"airflow/providers/google/cloud/transfers/s3_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/salesforce_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/sftp_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/sheets_to_gcs.py" = ["D100", "D102"]
"airflow/providers/google/cloud/transfers/sql_to_gcs.py" = ["D102"]
"airflow/providers/google/cloud/transfers/trino_to_gcs.py" = ["D100"]
"airflow/providers/google/cloud/triggers/__init__.py" = ["D104"]
"airflow/providers/google/cloud/triggers/bigquery.py" = ["D100"]
"airflow/providers/google/cloud/triggers/bigquery_dts.py" = ["D100"]
"airflow/providers/google/cloud/triggers/cloud_batch.py" = ["D100"]
"airflow/providers/google/cloud/triggers/cloud_build.py" = ["D100"]
"airflow/providers/google/cloud/triggers/cloud_composer.py" = ["D100", "D102"]
"airflow/providers/google/cloud/triggers/cloud_run.py" = ["D100", "D102"]
"airflow/providers/google/cloud/triggers/cloud_sql.py" = ["D102"]
"airflow/providers/google/cloud/triggers/cloud_storage_transfer_service.py" = ["D100", "D102"]
"airflow/providers/google/cloud/triggers/dataflow.py" = ["D100"]
"airflow/providers/google/cloud/triggers/datafusion.py" = ["D100"]
"airflow/providers/google/cloud/triggers/dataplex.py" = ["D102"]
"airflow/providers/google/cloud/triggers/dataproc.py" = ["D102"]
"airflow/providers/google/cloud/triggers/gcs.py" = ["D100"]
"airflow/providers/google/cloud/triggers/kubernetes_engine.py" = ["D100", "D102"]
"airflow/providers/google/cloud/triggers/mlengine.py" = ["D100"]
"airflow/providers/google/cloud/triggers/pubsub.py" = ["D102"]
"airflow/providers/google/cloud/triggers/vertex_ai.py" = ["D100", "D102"]
"airflow/providers/google/cloud/utils/__init__.py" = ["D104"]
"airflow/providers/google/cloud/utils/bigquery.py" = ["D100"]
"airflow/providers/google/cloud/utils/bigquery_get_data.py" = ["D100", "D103"]
"airflow/providers/google/cloud/utils/dataform.py" = ["D100", "D103"]
"airflow/providers/google/cloud/utils/datafusion.py" = ["D100", "D102"]
"airflow/providers/google/cloud/utils/dataproc.py" = ["D100"]
"airflow/providers/google/common/__init__.py" = ["D104"]
"airflow/providers/google/common/auth_backend/__init__.py" = ["D104"]
"airflow/providers/google/common/consts.py" = ["D100"]
"airflow/providers/google/common/hooks/__init__.py" = ["D104"]
"airflow/providers/google/common/links/__init__.py" = ["D104"]
"airflow/providers/google/common/links/storage.py" = ["D102"]
"airflow/providers/google/common/utils/__init__.py" = ["D104"]
"airflow/providers/google/common/utils/id_token_credentials.py" = ["D102"]
"airflow/providers/google/firebase/__init__.py" = ["D104"]
"airflow/providers/google/firebase/hooks/__init__.py" = ["D104"]
"airflow/providers/google/firebase/operators/__init__.py" = ["D104"]
"airflow/providers/google/firebase/operators/firestore.py" = ["D100", "D102"]
"airflow/providers/google/leveldb/__init__.py" = ["D104"]
"airflow/providers/google/leveldb/hooks/__init__.py" = ["D104"]
"airflow/providers/google/leveldb/operators/__init__.py" = ["D104"]
"airflow/providers/google/leveldb/operators/leveldb.py" = ["D100"]
"airflow/providers/google/marketing_platform/__init__.py" = ["D104"]
"airflow/providers/google/marketing_platform/hooks/__init__.py" = ["D104"]
"airflow/providers/google/marketing_platform/hooks/analytics.py" = ["D100"]
"airflow/providers/google/marketing_platform/hooks/analytics_admin.py" = ["D102"]
"airflow/providers/google/marketing_platform/links/__init__.py" = ["D104"]
"airflow/providers/google/marketing_platform/links/analytics_admin.py" = ["D100", "D102"]
"airflow/providers/google/marketing_platform/operators/__init__.py" = ["D104"]
"airflow/providers/google/marketing_platform/operators/analytics.py" = ["D102"]
"airflow/providers/google/marketing_platform/operators/analytics_admin.py" = ["D102"]
"airflow/providers/google/marketing_platform/operators/campaign_manager.py" = ["D102"]
"airflow/providers/google/marketing_platform/operators/display_video.py" = ["D102"]
"airflow/providers/google/marketing_platform/operators/search_ads.py" = ["D102"]
"airflow/providers/google/marketing_platform/sensors/__init__.py" = ["D104"]
"airflow/providers/google/marketing_platform/sensors/campaign_manager.py" = ["D102"]
"airflow/providers/google/marketing_platform/sensors/display_video.py" = ["D102"]
"airflow/providers/google/marketing_platform/sensors/search_ads.py" = ["D102"]
"airflow/providers/google/suite/__init__.py" = ["D104"]
"airflow/providers/google/suite/hooks/__init__.py" = ["D104"]
"airflow/providers/google/suite/operators/__init__.py" = ["D104"]
"airflow/providers/google/suite/operators/sheets.py" = ["D100", "D102"]
"airflow/providers/google/suite/sensors/__init__.py" = ["D104"]
"airflow/providers/google/suite/sensors/drive.py" = ["D102"]
"airflow/providers/google/suite/transfers/__init__.py" = ["D104"]
"airflow/providers/google/suite/transfers/gcs_to_gdrive.py" = ["D102"]
"airflow/providers/google/suite/transfers/gcs_to_sheets.py" = ["D100", "D102"]
"airflow/providers/google/suite/transfers/local_to_drive.py" = ["D102"]
"airflow/providers/google/suite/transfers/sql_to_sheets.py" = ["D100", "D102"]
"airflow/providers/grpc/__init__.py" = ["D104"]
"airflow/providers/grpc/hooks/__init__.py" = ["D104"]
"airflow/providers/grpc/hooks/grpc.py" = ["D102"]
"airflow/providers/grpc/operators/__init__.py" = ["D104"]
"airflow/providers/grpc/operators/grpc.py" = ["D100", "D102"]
"airflow/providers/hashicorp/__init__.py" = ["D104"]
"airflow/providers/hashicorp/hooks/__init__.py" = ["D104"]
"airflow/providers/hashicorp/secrets/__init__.py" = ["D104"]
"airflow/providers/http/__init__.py" = ["D104"]
"airflow/providers/http/hooks/__init__.py" = ["D104"]
"airflow/providers/http/hooks/http.py" = ["D100", "D102"]
"airflow/providers/http/operators/__init__.py" = ["D104"]
"airflow/providers/http/operators/http.py" = ["D100", "D102"]
"airflow/providers/http/sensors/__init__.py" = ["D104"]
"airflow/providers/http/sensors/http.py" = ["D100", "D102"]
"airflow/providers/http/triggers/__init__.py" = ["D104"]
"airflow/providers/http/triggers/http.py" = ["D100"]
"airflow/providers/imap/__init__.py" = ["D104"]
"airflow/providers/imap/hooks/__init__.py" = ["D104"]
"airflow/providers/imap/hooks/imap.py" = ["D105"]
"airflow/providers/imap/sensors/__init__.py" = ["D104"]
"airflow/providers/influxdb/__init__.py" = ["D104"]
"airflow/providers/influxdb/hooks/__init__.py" = ["D104"]
"airflow/providers/influxdb/hooks/influxdb.py" = ["D102"]
"airflow/providers/influxdb/operators/__init__.py" = ["D104"]
"airflow/providers/influxdb/operators/influxdb.py" = ["D100", "D102"]
"airflow/providers/jdbc/__init__.py" = ["D104"]
"airflow/providers/jdbc/hooks/__init__.py" = ["D104"]
"airflow/providers/jdbc/hooks/jdbc.py" = ["D100", "D102"]
"airflow/providers/jdbc/operators/__init__.py" = ["D104"]
"airflow/providers/jdbc/operators/jdbc.py" = ["D100"]
"airflow/providers/jenkins/__init__.py" = ["D104"]
"airflow/providers/jenkins/hooks/__init__.py" = ["D104"]
"airflow/providers/jenkins/hooks/jenkins.py" = ["D100", "D102"]
"airflow/providers/jenkins/operators/__init__.py" = ["D104"]
"airflow/providers/jenkins/operators/jenkins_job_trigger.py" = ["D100", "D102"]
"airflow/providers/jenkins/sensors/__init__.py" = ["D104"]
"airflow/providers/jenkins/sensors/jenkins.py" = ["D100", "D102"]
"airflow/providers/microsoft/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/fs/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/fs/adls.py" = ["D100", "D103"]
"airflow/providers/microsoft/azure/hooks/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/hooks/asb.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/hooks/base_azure.py" = ["D100"]
"airflow/providers/microsoft/azure/hooks/batch.py" = ["D100"]
"airflow/providers/microsoft/azure/hooks/container_instance.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/hooks/container_registry.py" = ["D102"]
"airflow/providers/microsoft/azure/hooks/container_volume.py" = ["D100"]
"airflow/providers/microsoft/azure/hooks/data_factory.py" = ["D102"]
"airflow/providers/microsoft/azure/hooks/data_lake.py" = ["D100"]
"airflow/providers/microsoft/azure/hooks/fileshare.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/hooks/synapse.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/log/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/log/wasb_task_handler.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/operators/adls.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/asb.py" = ["D100"]
"airflow/providers/microsoft/azure/operators/batch.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/container_instances.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/cosmos.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/data_factory.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/synapse.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/operators/wasb_delete_blob.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/secrets/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/sensors/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/sensors/cosmos.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/sensors/data_factory.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/sensors/wasb.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/transfers/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/transfers/azure_blob_to_gcs.py" = ["D100"]
"airflow/providers/microsoft/azure/transfers/local_to_adls.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/transfers/local_to_wasb.py" = ["D100"]
"airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py" = ["D100", "D102"]
"airflow/providers/microsoft/azure/transfers/sftp_to_wasb.py" = ["D102"]
"airflow/providers/microsoft/azure/triggers/__init__.py" = ["D104"]
"airflow/providers/microsoft/azure/triggers/data_factory.py" = ["D100"]
"airflow/providers/microsoft/azure/triggers/wasb.py" = ["D100"]
"airflow/providers/microsoft/azure/utils.py" = ["D100", "D102", "D103"]
"airflow/providers/microsoft/mssql/__init__.py" = ["D104"]
"airflow/providers/microsoft/mssql/hooks/__init__.py" = ["D104"]
"airflow/providers/microsoft/mssql/hooks/mssql.py" = ["D102"]
"airflow/providers/microsoft/mssql/operators/__init__.py" = ["D104"]
"airflow/providers/microsoft/mssql/operators/mssql.py" = ["D100"]
"airflow/providers/microsoft/psrp/__init__.py" = ["D104"]
"airflow/providers/microsoft/psrp/hooks/__init__.py" = ["D104"]
"airflow/providers/microsoft/psrp/hooks/psrp.py" = ["D100", "D105"]
"airflow/providers/microsoft/psrp/operators/__init__.py" = ["D104"]
"airflow/providers/microsoft/psrp/operators/psrp.py" = ["D100", "D102"]
"airflow/providers/microsoft/winrm/__init__.py" = ["D104"]
"airflow/providers/microsoft/winrm/hooks/__init__.py" = ["D104"]
"airflow/providers/microsoft/winrm/hooks/winrm.py" = ["D102"]
"airflow/providers/microsoft/winrm/operators/__init__.py" = ["D104"]
"airflow/providers/microsoft/winrm/operators/winrm.py" = ["D100", "D102"]
"airflow/providers/mongo/__init__.py" = ["D104"]
"airflow/providers/mongo/hooks/__init__.py" = ["D104"]
"airflow/providers/mongo/hooks/mongo.py" = ["D105"]
"airflow/providers/mongo/sensors/__init__.py" = ["D104"]
"airflow/providers/mongo/sensors/mongo.py" = ["D100", "D102"]
"airflow/providers/mysql/__init__.py" = ["D104"]
"airflow/providers/mysql/hooks/__init__.py" = ["D104"]
"airflow/providers/mysql/operators/__init__.py" = ["D104"]
"airflow/providers/mysql/operators/mysql.py" = ["D100"]
"airflow/providers/mysql/transfers/__init__.py" = ["D104"]
"airflow/providers/mysql/transfers/presto_to_mysql.py" = ["D100", "D102"]
"airflow/providers/mysql/transfers/s3_to_mysql.py" = ["D100"]
"airflow/providers/mysql/transfers/trino_to_mysql.py" = ["D100", "D102"]
"airflow/providers/mysql/transfers/vertica_to_mysql.py" = ["D100", "D102"]
"airflow/providers/neo4j/__init__.py" = ["D104"]
"airflow/providers/neo4j/hooks/__init__.py" = ["D104"]
"airflow/providers/neo4j/operators/__init__.py" = ["D104"]
"airflow/providers/neo4j/operators/neo4j.py" = ["D100", "D102"]
"airflow/providers/odbc/__init__.py" = ["D104"]
"airflow/providers/odbc/hooks/__init__.py" = ["D104"]
"airflow/providers/odbc/hooks/odbc.py" = ["D102"]
"airflow/providers/openai/__init__.py" = ["D104"]
"airflow/providers/openai/hooks/__init__.py" = ["D104"]
"airflow/providers/openai/hooks/openai.py" = ["D100", "D102"]
"airflow/providers/openai/operators/__init__.py" = ["D104"]
"airflow/providers/openai/operators/openai.py" = ["D100", "D102"]
"airflow/providers/openfaas/__init__.py" = ["D104"]
"airflow/providers/openfaas/hooks/__init__.py" = ["D104"]
"airflow/providers/openfaas/hooks/openfaas.py" = ["D100", "D102"]
"airflow/providers/openlineage/__init__.py" = ["D104"]
"airflow/providers/openlineage/extractors/__init__.py" = ["D104"]
"airflow/providers/openlineage/extractors/base.py" = ["D100", "D102"]
"airflow/providers/openlineage/extractors/bash.py" = ["D100", "D102"]
"airflow/providers/openlineage/extractors/manager.py" = ["D100", "D102", "D103"]
"airflow/providers/openlineage/extractors/python.py" = ["D100", "D102"]
"airflow/providers/openlineage/plugins/__init__.py" = ["D104"]
"airflow/providers/openlineage/plugins/adapter.py" = ["D100", "D102"]
"airflow/providers/openlineage/plugins/facets.py" = ["D100", "D102"]
"airflow/providers/openlineage/plugins/listener.py" = ["D100", "D102"]
"airflow/providers/openlineage/plugins/macros.py" = ["D100"]
"airflow/providers/openlineage/plugins/openlineage.py" = ["D100"]
"airflow/providers/openlineage/sqlparser.py" = ["D100", "D102", "D103"]
"airflow/providers/openlineage/utils/__init__.py" = ["D104"]
"airflow/providers/openlineage/utils/sql.py" = ["D100", "D102"]
"airflow/providers/openlineage/utils/utils.py" = ["D100", "D102", "D103"]
"airflow/providers/opensearch/__init__.py" = ["D104"]
"airflow/providers/opensearch/hooks/__init__.py" = ["D104"]
"airflow/providers/opensearch/hooks/opensearch.py" = ["D100", "D102"]
"airflow/providers/opensearch/operators/__init__.py" = ["D104"]
"airflow/providers/opensearch/operators/opensearch.py" = ["D100"]
"airflow/providers/opsgenie/__init__.py" = ["D104"]
"airflow/providers/opsgenie/hooks/__init__.py" = ["D104"]
"airflow/providers/opsgenie/hooks/opsgenie.py" = ["D100"]
"airflow/providers/opsgenie/notifications/__init__.py" = ["D104"]
"airflow/providers/opsgenie/notifications/opsgenie.py" = ["D100"]
"airflow/providers/opsgenie/operators/__init__.py" = ["D104"]
"airflow/providers/opsgenie/operators/opsgenie.py" = ["D100"]
"airflow/providers/opsgenie/typing/__init__.py" = ["D104"]
"airflow/providers/opsgenie/typing/opsgenie.py" = ["D100"]
"airflow/providers/oracle/__init__.py" = ["D104"]
"airflow/providers/oracle/hooks/__init__.py" = ["D104"]
"airflow/providers/oracle/hooks/oracle.py" = ["D100"]
"airflow/providers/oracle/operators/__init__.py" = ["D104"]
"airflow/providers/oracle/operators/oracle.py" = ["D100", "D102"]
"airflow/providers/oracle/transfers/__init__.py" = ["D104"]
"airflow/providers/oracle/transfers/oracle_to_oracle.py" = ["D100", "D102"]
"airflow/providers/pagerduty/__init__.py" = ["D104"]
"airflow/providers/pagerduty/hooks/__init__.py" = ["D104"]
"airflow/providers/pagerduty/hooks/pagerduty.py" = ["D102"]
"airflow/providers/pagerduty/hooks/pagerduty_events.py" = ["D102"]
"airflow/providers/pagerduty/notifications/__init__.py" = ["D104"]
"airflow/providers/pagerduty/notifications/pagerduty.py" = ["D100"]
"airflow/providers/papermill/__init__.py" = ["D104"]
"airflow/providers/papermill/hooks/__init__.py" = ["D104"]
"airflow/providers/papermill/hooks/kernel.py" = ["D100", "D102"]
"airflow/providers/papermill/operators/__init__.py" = ["D104"]
"airflow/providers/papermill/operators/papermill.py" = ["D100", "D102"]
"airflow/providers/pgvector/__init__.py" = ["D104"]
"airflow/providers/pgvector/hooks/__init__.py" = ["D104"]
"airflow/providers/pgvector/hooks/pgvector.py" = ["D100"]
"airflow/providers/pgvector/operators/__init__.py" = ["D104"]
"airflow/providers/pgvector/operators/pgvector.py" = ["D100", "D102"]
"airflow/providers/pinecone/__init__.py" = ["D104"]
"airflow/providers/pinecone/hooks/__init__.py" = ["D104"]
"airflow/providers/pinecone/hooks/pinecone.py" = ["D102"]
"airflow/providers/pinecone/operators/__init__.py" = ["D104"]
"airflow/providers/pinecone/operators/pinecone.py" = ["D100"]
"airflow/providers/postgres/__init__.py" = ["D104"]
"airflow/providers/postgres/hooks/__init__.py" = ["D104"]
"airflow/providers/postgres/hooks/postgres.py" = ["D100", "D102"]
"airflow/providers/postgres/operators/__init__.py" = ["D104"]
"airflow/providers/postgres/operators/postgres.py" = ["D100"]
"airflow/providers/presto/__init__.py" = ["D104"]
"airflow/providers/presto/hooks/__init__.py" = ["D104"]
"airflow/providers/presto/hooks/presto.py" = ["D100", "D102"]
"airflow/providers/presto/transfers/__init__.py" = ["D104"]
"airflow/providers/presto/transfers/gcs_to_presto.py" = ["D102"]
"airflow/providers/qdrant/__init__.py" = ["D104"]
"airflow/providers/qdrant/hooks/__init__.py" = ["D104"]
"airflow/providers/qdrant/hooks/qdrant.py" = ["D100"]
"airflow/providers/qdrant/operators/__init__.py" = ["D104"]
"airflow/providers/qdrant/operators/qdrant.py" = ["D100"]
"airflow/providers/redis/__init__.py" = ["D104"]
"airflow/providers/redis/hooks/__init__.py" = ["D104"]
"airflow/providers/redis/log/__init__.py" = ["D104"]
"airflow/providers/redis/log/redis_task_handler.py" = ["D100", "D102"]
"airflow/providers/redis/operators/__init__.py" = ["D104"]
"airflow/providers/redis/operators/redis_publish.py" = ["D100"]
"airflow/providers/redis/sensors/__init__.py" = ["D104"]
"airflow/providers/redis/sensors/redis_key.py" = ["D100", "D102"]
"airflow/providers/redis/sensors/redis_pub_sub.py" = ["D100", "D102"]
"airflow/providers/salesforce/__init__.py" = ["D104"]
"airflow/providers/salesforce/hooks/__init__.py" = ["D104"]
"airflow/providers/salesforce/operators/__init__.py" = ["D104"]
"airflow/providers/salesforce/operators/bulk.py" = ["D100"]
"airflow/providers/salesforce/operators/salesforce_apex_rest.py" = ["D100"]
"airflow/providers/samba/__init__.py" = ["D104"]
"airflow/providers/samba/hooks/__init__.py" = ["D104"]
"airflow/providers/samba/hooks/samba.py" = ["D100", "D102", "D105"]
"airflow/providers/samba/transfers/__init__.py" = ["D104"]
"airflow/providers/samba/transfers/gcs_to_samba.py" = ["D102"]
"airflow/providers/segment/__init__.py" = ["D104"]
"airflow/providers/segment/hooks/__init__.py" = ["D104"]
"airflow/providers/segment/hooks/segment.py" = ["D102"]
"airflow/providers/segment/operators/__init__.py" = ["D104"]
"airflow/providers/segment/operators/segment_track_event.py" = ["D100", "D102"]
"airflow/providers/sendgrid/__init__.py" = ["D104"]
"airflow/providers/sendgrid/utils/__init__.py" = ["D104"]
"airflow/providers/sftp/__init__.py" = ["D104"]
"airflow/providers/sftp/decorators/__init__.py" = ["D104"]
"airflow/providers/sftp/decorators/sensors/__init__.py" = ["D104"]
"airflow/providers/sftp/decorators/sensors/sftp.py" = ["D100"]
"airflow/providers/sftp/hooks/__init__.py" = ["D104"]
"airflow/providers/sftp/hooks/sftp.py" = ["D102"]
"airflow/providers/sftp/operators/__init__.py" = ["D104"]
"airflow/providers/sftp/operators/sftp.py" = ["D102"]
"airflow/providers/sftp/sensors/__init__.py" = ["D104"]
"airflow/providers/sftp/sensors/sftp.py" = ["D102"]
"airflow/providers/sftp/triggers/__init__.py" = ["D104"]
"airflow/providers/sftp/triggers/sftp.py" = ["D100"]
"airflow/providers/singularity/__init__.py" = ["D104"]
"airflow/providers/singularity/operators/__init__.py" = ["D104"]
"airflow/providers/singularity/operators/singularity.py" = ["D100", "D102"]
"airflow/providers/slack/__init__.py" = ["D104"]
"airflow/providers/slack/hooks/__init__.py" = ["D104"]
"airflow/providers/slack/hooks/slack.py" = ["D100"]
"airflow/providers/slack/hooks/slack_webhook.py" = ["D100"]
"airflow/providers/slack/notifications/__init__.py" = ["D104"]
"airflow/providers/slack/notifications/slack.py" = ["D100"]
"airflow/providers/slack/notifications/slack_webhook.py" = ["D100"]
"airflow/providers/slack/operators/__init__.py" = ["D104"]
"airflow/providers/slack/operators/slack.py" = ["D100", "D102"]
"airflow/providers/slack/operators/slack_webhook.py" = ["D100"]
"airflow/providers/slack/transfers/__init__.py" = ["D104"]
"airflow/providers/slack/transfers/base_sql_to_slack.py" = ["D100"]
"airflow/providers/slack/transfers/sql_to_slack.py" = ["D100", "D102"]
"airflow/providers/slack/transfers/sql_to_slack_webhook.py" = ["D100", "D102"]
"airflow/providers/slack/utils/__init__.py" = ["D104"]
"airflow/providers/smtp/__init__.py" = ["D104"]
"airflow/providers/smtp/hooks/__init__.py" = ["D104"]
"airflow/providers/smtp/hooks/smtp.py" = ["D102", "D105"]
"airflow/providers/smtp/notifications/__init__.py" = ["D104"]
"airflow/providers/smtp/notifications/smtp.py" = ["D100"]
"airflow/providers/smtp/notifications/templates/__init__.py" = ["D104"]
"airflow/providers/smtp/operators/__init__.py" = ["D104"]
"airflow/providers/smtp/operators/smtp.py" = ["D100", "D102"]
"airflow/providers/snowflake/__init__.py" = ["D104"]
"airflow/providers/snowflake/hooks/__init__.py" = ["D104"]
"airflow/providers/snowflake/hooks/snowflake.py" = ["D100", "D102"]
"airflow/providers/snowflake/hooks/snowflake_sql_api.py" = ["D100"]
"airflow/providers/snowflake/operators/__init__.py" = ["D104"]
"airflow/providers/snowflake/operators/snowflake.py" = ["D100"]
"airflow/providers/snowflake/transfers/__init__.py" = ["D104"]
"airflow/providers/snowflake/transfers/copy_into_snowflake.py" = ["D102"]
"airflow/providers/snowflake/triggers/__init__.py" = ["D104"]
"airflow/providers/snowflake/triggers/snowflake_trigger.py" = ["D100"]
"airflow/providers/snowflake/utils/__init__.py" = ["D104"]
"airflow/providers/snowflake/utils/common.py" = ["D100"]
"airflow/providers/snowflake/utils/sql_api_generate_jwt.py" = ["D100"]
"airflow/providers/sqlite/__init__.py" = ["D104"]
"airflow/providers/sqlite/hooks/__init__.py" = ["D104"]
"airflow/providers/sqlite/hooks/sqlite.py" = ["D100"]
"airflow/providers/sqlite/operators/__init__.py" = ["D104"]
"airflow/providers/sqlite/operators/sqlite.py" = ["D100"]
"airflow/providers/ssh/__init__.py" = ["D104"]
"airflow/providers/ssh/hooks/__init__.py" = ["D104"]
"airflow/providers/ssh/hooks/ssh.py" = ["D102", "D105"]
"airflow/providers/ssh/operators/__init__.py" = ["D104"]
"airflow/providers/ssh/operators/ssh.py" = ["D100", "D102"]
"airflow/providers/tableau/__init__.py" = ["D104"]
"airflow/providers/tableau/hooks/__init__.py" = ["D104"]
"airflow/providers/tableau/hooks/tableau.py" = ["D100", "D105"]
"airflow/providers/tableau/operators/__init__.py" = ["D104"]
"airflow/providers/tableau/operators/tableau.py" = ["D100"]
"airflow/providers/tableau/sensors/__init__.py" = ["D104"]
"airflow/providers/tableau/sensors/tableau.py" = ["D100"]
"airflow/providers/tabular/__init__.py" = ["D104"]
"airflow/providers/tabular/hooks/__init__.py" = ["D104"]
"airflow/providers/tabular/hooks/tabular.py" = ["D100", "D102"]
"airflow/providers/telegram/__init__.py" = ["D104"]
"airflow/providers/telegram/hooks/__init__.py" = ["D104"]
"airflow/providers/telegram/operators/__init__.py" = ["D104"]
"airflow/providers/teradata/__init__.py" = ["D104"]
"airflow/providers/teradata/hooks/__init__.py" = ["D104"]
"airflow/providers/teradata/operators/__init__.py" = ["D104"]
"airflow/providers/teradata/operators/teradata.py" = ["D100"]
"airflow/providers/teradata/transfers/__init__.py" = ["D104"]
"airflow/providers/teradata/transfers/teradata_to_teradata.py" = ["D100", "D102"]
"airflow/providers/trino/__init__.py" = ["D104"]
"airflow/providers/trino/hooks/__init__.py" = ["D104"]
"airflow/providers/trino/hooks/trino.py" = ["D100", "D102"]
"airflow/providers/trino/operators/__init__.py" = ["D104"]
"airflow/providers/trino/operators/trino.py" = ["D102"]
"airflow/providers/trino/transfers/__init__.py" = ["D104"]
"airflow/providers/trino/transfers/gcs_to_trino.py" = ["D102"]
"airflow/providers/vertica/__init__.py" = ["D104"]
"airflow/providers/vertica/hooks/__init__.py" = ["D104"]
"airflow/providers/vertica/hooks/vertica.py" = ["D100"]
"airflow/providers/vertica/operators/__init__.py" = ["D104"]
"airflow/providers/vertica/operators/vertica.py" = ["D100"]
"airflow/providers/weaviate/__init__.py" = ["D104"]
"airflow/providers/weaviate/hooks/__init__.py" = ["D104"]
"airflow/providers/weaviate/hooks/weaviate.py" = ["D100", "D102", "D103"]
"airflow/providers/weaviate/operators/__init__.py" = ["D104"]
"airflow/providers/weaviate/operators/weaviate.py" = ["D100", "D102"]
"airflow/providers/yandex/__init__.py" = ["D104"]
"airflow/providers/yandex/hooks/__init__.py" = ["D104"]
"airflow/providers/yandex/hooks/yandex.py" = ["D100", "D102"]
"airflow/providers/yandex/hooks/yandexcloud_dataproc.py" = ["D100"]
"airflow/providers/yandex/operators/__init__.py" = ["D104"]
"airflow/providers/yandex/operators/yandexcloud_dataproc.py" = ["D100", "D102"]
"airflow/providers/yandex/secrets/__init__.py" = ["D104"]
"airflow/providers/yandex/utils/__init__.py" = ["D104"]
"airflow/providers/yandex/utils/credentials.py" = ["D100"]
"airflow/providers/yandex/utils/defaults.py" = ["D100"]
"airflow/providers/yandex/utils/fields.py" = ["D100"]
"airflow/providers/yandex/utils/user_agent.py" = ["D100"]
"airflow/providers/zendesk/__init__.py" = ["D104"]
"airflow/providers/zendesk/hooks/__init__.py" = ["D104"]
"airflow/providers/zendesk/hooks/zendesk.py" = ["D100", "D102"]
"airflow/providers_manager.py" = ["D102", "D105"]
"airflow/secrets/base_secrets.py" = ["D100"]
"airflow/secrets/cache.py" = ["D100"]
"airflow/secrets/environment_variables.py" = ["D102"]
"airflow/secrets/local_filesystem.py" = ["D102"]
"airflow/secrets/metastore.py" = ["D102"]
"airflow/security/__init__.py" = ["D104"]
"airflow/security/permissions.py" = ["D100"]
"airflow/sensors/base.py" = ["D100", "D102", "D105"]
"airflow/sensors/bash.py" = ["D100"]
"airflow/sensors/date_time.py" = ["D100", "D102"]
"airflow/sensors/external_task.py" = ["D100", "D102", "D105"]
"airflow/sensors/filesystem.py" = ["D100", "D102"]
"airflow/sensors/python.py" = ["D100", "D102"]
"airflow/sensors/time_delta.py" = ["D100", "D102"]
"airflow/sensors/time_sensor.py" = ["D100", "D102"]
"airflow/sensors/weekday.py" = ["D100", "D102"]
"airflow/sentry.py" = ["D102"]
"airflow/serialization/pydantic/__init__.py" = ["D104"]
"airflow/serialization/pydantic/dag.py" = ["D100", "D103"]
"airflow/serialization/pydantic/dag_run.py" = ["D100", "D102"]
"airflow/serialization/pydantic/dataset.py" = ["D100"]
"airflow/serialization/pydantic/job.py" = ["D100", "D102", "D103"]
"airflow/serialization/pydantic/taskinstance.py" = ["D100", "D102", "D103"]
"airflow/serialization/pydantic/tasklog.py" = ["D100"]
"airflow/serialization/serde.py" = ["D100", "D103"]
"airflow/serialization/serialized_objects.py" = ["D102"]
"airflow/serialization/serializers/__init__.py" = ["D104"]
"airflow/serialization/serializers/bignum.py" = ["D100", "D103"]
"airflow/serialization/serializers/builtin.py" = ["D100", "D103"]
"airflow/serialization/serializers/datetime.py" = ["D100", "D103"]
"airflow/serialization/serializers/deltalake.py" = ["D100", "D103"]
"airflow/serialization/serializers/iceberg.py" = ["D100", "D103"]
"airflow/serialization/serializers/kubernetes.py" = ["D100", "D103"]
"airflow/serialization/serializers/numpy.py" = ["D100", "D103"]
"airflow/serialization/serializers/pandas.py" = ["D100", "D103"]
"airflow/serialization/serializers/timezone.py" = ["D100", "D103"]
"airflow/settings.py" = ["D100", "D102", "D103"]
"airflow/stats.py" = ["D100"]
"airflow/task/__init__.py" = ["D104"]
"airflow/task/task_runner/__init__.py" = ["D104"]
"airflow/task/task_runner/cgroup_task_runner.py" = ["D102"]
"airflow/task/task_runner/standard_task_runner.py" = ["D102"]
"airflow/template/__init__.py" = ["D104"]
"airflow/template/templater.py" = ["D100", "D102"]
"airflow/templates.py" = ["D100"]
"airflow/ti_deps/__init__.py" = ["D104"]
"airflow/ti_deps/dep_context.py" = ["D100"]
"airflow/ti_deps/dependencies_deps.py" = ["D100"]
"airflow/ti_deps/dependencies_states.py" = ["D100"]
"airflow/ti_deps/deps/base_ti_dep.py" = ["D100", "D105"]
"airflow/ti_deps/deps/dag_ti_slots_available_dep.py" = ["D100"]
"airflow/ti_deps/deps/dag_unpaused_dep.py" = ["D100"]
"airflow/ti_deps/deps/dagrun_exists_dep.py" = ["D100"]
"airflow/ti_deps/deps/exec_date_after_start_date_dep.py" = ["D100"]
"airflow/ti_deps/deps/mapped_task_expanded.py" = ["D100"]
"airflow/ti_deps/deps/not_in_retry_period_dep.py" = ["D100"]
"airflow/ti_deps/deps/not_previously_skipped_dep.py" = ["D100"]
"airflow/ti_deps/deps/prev_dagrun_dep.py" = ["D100"]
"airflow/ti_deps/deps/ready_to_reschedule.py" = ["D100"]
"airflow/ti_deps/deps/runnable_exec_date_dep.py" = ["D100"]
"airflow/ti_deps/deps/task_concurrency_dep.py" = ["D100"]
"airflow/ti_deps/deps/task_not_running_dep.py" = ["D105"]
"airflow/ti_deps/deps/trigger_rule_dep.py" = ["D100"]
"airflow/ti_deps/deps/valid_state_dep.py" = ["D100", "D105"]
"airflow/timetables/__init__.py" = ["D104"]
"airflow/timetables/base.py" = ["D100", "D102"]
"airflow/timetables/datasets.py" = ["D100", "D102"]
"airflow/timetables/events.py" = ["D100", "D102", "D105"]
"airflow/timetables/interval.py" = ["D100", "D102"]
"airflow/timetables/simple.py" = ["D100", "D102"]
"airflow/timetables/trigger.py" = ["D100", "D102"]
"airflow/triggers/__init__.py" = ["D104"]
"airflow/triggers/base.py" = ["D100", "D105"]
"airflow/triggers/external_task.py" = ["D100"]
"airflow/triggers/file.py" = ["D100"]
"airflow/triggers/temporal.py" = ["D100", "D102"]
"airflow/triggers/testing.py" = ["D100", "D102"]
"airflow/utils/__init__.py" = ["D104"]
"airflow/utils/airflow_flask_app.py" = ["D100", "D103"]
"airflow/utils/cli.py" = ["D103"]
"airflow/utils/code_utils.py" = ["D100"]
"airflow/utils/compression.py" = ["D100"]
"airflow/utils/configuration.py" = ["D100"]
"airflow/utils/context.py" = ["D102", "D105"]
"airflow/utils/dag_edges.py" = ["D100"]
"airflow/utils/dag_parsing_context.py" = ["D100"]
"airflow/utils/dates.py" = ["D100"]
"airflow/utils/db.py" = ["D100", "D103", "D105"]
"airflow/utils/decorators.py" = ["D100", "D103"]
"airflow/utils/deprecation_tools.py" = ["D100"]
"airflow/utils/docs.py" = ["D100"]
"airflow/utils/edgemodifier.py" = ["D100", "D102"]
"airflow/utils/email.py" = ["D100"]
"airflow/utils/empty_set.py" = ["D100"]
"airflow/utils/entry_points.py" = ["D100"]
"airflow/utils/event_scheduler.py" = ["D100"]
"airflow/utils/file.py" = ["D100"]
"airflow/utils/hashlib_wrapper.py" = ["D100"]
"airflow/utils/helpers.py" = ["D100"]
"airflow/utils/json.py" = ["D100", "D102"]
"airflow/utils/jwt_signer.py" = ["D100", "D102"]
"airflow/utils/log/__init__.py" = ["D104"]
"airflow/utils/log/action_logger.py" = ["D100", "D103"]
"airflow/utils/log/colored_log.py" = ["D102"]
"airflow/utils/log/file_processor_handler.py" = ["D100", "D102"]
"airflow/utils/log/file_task_handler.py" = ["D102"]
"airflow/utils/log/json_formatter.py" = ["D102"]
"airflow/utils/log/log_reader.py" = ["D100"]
"airflow/utils/log/logging_mixin.py" = ["D100"]
"airflow/utils/log/non_caching_file_handler.py" = ["D100", "D103"]
"airflow/utils/log/secrets_masker.py" = ["D102", "D105"]
"airflow/utils/log/task_context_logger.py" = ["D100"]
"airflow/utils/log/timezone_aware.py" = ["D100"]
"airflow/utils/log/trigger_handler.py" = ["D100", "D102"]
"airflow/utils/mixins.py" = ["D100"]
"airflow/utils/module_loading.py" = ["D100", "D103"]
"airflow/utils/net.py" = ["D100"]
"airflow/utils/operator_helpers.py" = ["D100", "D102"]
"airflow/utils/operator_resources.py" = ["D100", "D102", "D105"]
"airflow/utils/orm_event_handlers.py" = ["D100"]
"airflow/utils/providers_configuration_loader.py" = ["D100"]
"airflow/utils/pydantic.py" = ["D100", "D103"]
"airflow/utils/python_virtualenv.py" = ["D103"]
"airflow/utils/retries.py" = ["D100"]
"airflow/utils/scheduler_health.py" = ["D100", "D102"]
"airflow/utils/sensor_helper.py" = ["D100"]
"airflow/utils/serve_logs.py" = ["D102", "D103"]
"airflow/utils/session.py" = ["D100"]
"airflow/utils/setup_teardown.py" = ["D100", "D102"]
"airflow/utils/singleton.py" = ["D100", "D102"]
"airflow/utils/sqlalchemy.py" = ["D100", "D102", "D105"]
"airflow/utils/state.py" = ["D100", "D105"]
"airflow/utils/task_group.py" = ["D102", "D105"]
"airflow/utils/task_instance_session.py" = ["D100", "D103"]
"airflow/utils/template.py" = ["D100"]
"airflow/utils/timeout.py" = ["D100", "D105"]
"airflow/utils/timezone.py" = ["D100"]
"airflow/utils/trigger_rule.py" = ["D100", "D105"]
"airflow/utils/types.py" = ["D100", "D102", "D105"]
"airflow/utils/weight_rule.py" = ["D100", "D105"]
"airflow/utils/xcom.py" = ["D100"]
"airflow/version.py" = ["D100"]
"airflow/www/__init__.py" = ["D104"]
"airflow/www/api/__init__.py" = ["D104"]
"airflow/www/api/experimental/__init__.py" = ["D104"]
"airflow/www/api/experimental/endpoints.py" = ["D100"]
"airflow/www/app.py" = ["D100"]
"airflow/www/auth.py" = ["D100", "D103"]
"airflow/www/blueprints.py" = ["D100"]
"airflow/www/decorators.py" = ["D100"]
"airflow/www/extensions/__init__.py" = ["D104"]
"airflow/www/extensions/init_appbuilder.py" = ["D100", "D102"]
"airflow/www/extensions/init_appbuilder_links.py" = ["D100"]
"airflow/www/extensions/init_auth_manager.py" = ["D100"]
"airflow/www/extensions/init_cache.py" = ["D100", "D103"]
"airflow/www/extensions/init_dagbag.py" = ["D100"]
"airflow/www/extensions/init_jinja_globals.py" = ["D100"]
"airflow/www/extensions/init_manifest_files.py" = ["D100"]
"airflow/www/extensions/init_robots.py" = ["D100"]
"airflow/www/extensions/init_security.py" = ["D100", "D103"]
"airflow/www/extensions/init_session.py" = ["D100"]
"airflow/www/extensions/init_views.py" = ["D100"]
"airflow/www/extensions/init_wsgi_middlewares.py" = ["D100"]
"airflow/www/fab_security/__init__.py" = ["D104"]
"airflow/www/fab_security/manager.py" = ["D100"]
"airflow/www/forms.py" = ["D100", "D102"]
"airflow/www/gunicorn_config.py" = ["D100", "D103"]
"airflow/www/security.py" = ["D100"]
"airflow/www/security_appless.py" = ["D100"]
"airflow/www/security_manager.py" = ["D100", "D102"]
"airflow/www/session.py" = ["D100"]
"airflow/www/utils.py" = ["D100", "D102", "D103"]
"airflow/www/validators.py" = ["D100", "D102"]
"airflow/www/views.py" = ["D100", "D102", "D103"]
"airflow/www/widgets.py" = ["D100", "D102"]
"clients/python/test_python_client.py" = ["D100"]
"hatch_build.py" = ["D100", "D102"]


[tool.ruff.lint.flake8-tidy-imports]
# Ban certain modules from being imported at module level, instead requiring
# that they're imported lazily (e.g., within a function definition).
banned-module-level-imports = ["numpy", "pandas"]

[tool.ruff.lint.flake8-tidy-imports.banned-api]
"airflow.AirflowException".msg = "Use airflow.exceptions.AirflowException instead."
"airflow.Dataset".msg = "Use airflow.datasets.Dataset instead."
"airflow.models.baseoperator.BaseOperatorLink".msg = "Use airflow.models.baseoperatorlink.BaseOperatorLink"
# Uses deprecated in Python 3.12 `datetime.datetime.utcfromtimestamp`
"pendulum.from_timestamp".msg = "Use airflow.utils.timezone.from_timestamp"

[tool.ruff.lint.flake8-type-checking]
exempt-modules = ["typing", "typing_extensions"]

## pytest settings ##
[tool.pytest.ini_options]
# * Disable `flaky` plugin for pytest. This plugin conflicts with `rerunfailures` because provide same marker.
# * Disable `nose` builtin plugin for pytest. This feature deprecated in 7.2 and will be removed in pytest>=8
# * And we focus on use native pytest capabilities rather than adopt another frameworks.
addopts = "-rasl --verbosity=2 -p no:flaky -p no:nose --asyncio-mode=strict"
norecursedirs = [
    ".eggs",
    "airflow",
    "tests/dags_with_system_exit",
    "tests/test_utils",
    "tests/dags_corrupted",
    "tests/dags",
    "tests/system/providers/google/cloud/dataproc/resources",
    "tests/system/providers/google/cloud/gcs/resources",
]
log_level = "INFO"
filterwarnings = [
    "error::pytest.PytestCollectionWarning",
    "ignore::DeprecationWarning:flask_appbuilder.filemanager",
    "ignore::DeprecationWarning:flask_appbuilder.widgets",
    # https://github.com/dpgaspar/Flask-AppBuilder/pull/1940
    "ignore::DeprecationWarning:flask_sqlalchemy",
    # https://github.com/dpgaspar/Flask-AppBuilder/pull/1903
    "ignore::DeprecationWarning:apispec.utils",
]
python_files = [
    "test_*.py",
    "example_*.py",
]
testpaths = [
    "tests",
]


## coverage.py settings ##
[tool.coverage.run]
branch = true
relative_files = true
source = ["airflow"]
omit = [
    "airflow/_vendor/**",
    "airflow/contrib/**",
    "airflow/example_dags/**",
    "airflow/migrations/**",
    "airflow/providers/**/example_dags/**",
    "airflow/www/node_modules/**",
    "airflow/providers/google/ads/_vendor/**",
]

[tool.coverage.report]
skip_empty = true
exclude_also = [
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "@(abc\\.)?abstractmethod",
    "@(typing(_extensions)?\\.)?overload",
    "if (typing(_extensions)?\\.)?TYPE_CHECKING:"
]


## mypy settings ##
[tool.mypy]
ignore_missing_imports = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = false
plugins = [
    "dev/mypy/plugin/decorators.py",
    "dev/mypy/plugin/outputs.py",
]
pretty = true
show_error_codes = true
disable_error_code = [
    "annotation-unchecked",
]

[[tool.mypy.overrides]]
module="airflow.config_templates.default_webserver_config"
disable_error_code = [
    "var-annotated",
]

[[tool.mypy.overrides]]
module="airflow.migrations.*"
ignore_errors = true

[[tool.mypy.overrides]]
module= [
    "google.cloud.*",
    "azure.*",
]
no_implicit_optional = false

[[tool.mypy.overrides]]
module=[
    "referencing.*",
    # Beam has some old type annotations, and they introduced an error recently with bad signature of
    # a function. This is captured in https://github.com/apache/beam/issues/29927
    # and we should remove this exclusion when it is fixed.
    "apache_beam.*"
]
ignore_errors = true
