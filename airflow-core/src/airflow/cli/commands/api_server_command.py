# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
"""FastAPI API command."""

from __future__ import annotations

import logging
import os
import signal
import subprocess
import sys
import textwrap
import threading
from collections.abc import Callable
from functools import wraps
from typing import TYPE_CHECKING, TypeVar

import uvicorn

from airflow import settings
from airflow.cli.commands.daemon_utils import run_command_with_daemon_option
from airflow.configuration import conf
from airflow.exceptions import AirflowConfigException
from airflow.typing_compat import ParamSpec
from airflow.utils import cli as cli_utils
from airflow.utils.memray_utils import MemrayTraceComponents, enable_memray_trace
from airflow.utils.providers_configuration_loader import providers_configuration_loaded

PS = ParamSpec("PS")
RT = TypeVar("RT")

log = logging.getLogger(__name__)

if TYPE_CHECKING:
    from argparse import Namespace

# This shouldn't be necessary but there seems to be an issue in uvloop that causes bad file descriptor
# errors when shutting down workers. Despite the 'closed' status of the issue it is not solved,
# more info here: https://github.com/benoitc/gunicorn/issues/1877#issuecomment-1911136399


def _build_gunicorn_command(
    host: str,
    port: int,
    num_workers: int,
    worker_timeout: int,
    ssl_cert: str | None,
    ssl_key: str | None,
    log_level: str,
    access_log_enabled: bool,
    proxy_headers: bool,
) -> list[str]:
    """
    Build the gunicorn command line arguments.

    Uses uvicorn.workers.UvicornWorker as the worker class to run the ASGI app.
    """
    cmd = [
        "gunicorn",
        "airflow.api_fastapi.main:app",
        "--worker-class",
        "uvicorn.workers.UvicornWorker",
        "--config",
        "python:airflow.api_fastapi.gunicorn_config",
        "--bind",
        f"{host}:{port}",
        "--workers",
        str(num_workers),
        "--timeout",
        str(worker_timeout),
        "--graceful-timeout",
        str(worker_timeout),
        "--keep-alive",
        str(worker_timeout),
        "--log-level",
        log_level,
        # Preload app to share memory across workers via copy-on-write
        "--preload",
    ]

    if ssl_cert and ssl_key:
        cmd.extend(["--certfile", ssl_cert, "--keyfile", ssl_key])

    if not access_log_enabled:
        cmd.append("--disable-redirect-access-to-syslog")

    if proxy_headers:
        cmd.extend(["--forwarded-allow-ips", "*"])

    return cmd


def _run_api_server_with_gunicorn(
    args,
    apps: str,
    num_workers: int,
    worker_timeout: int,
    proxy_headers: bool,
) -> None:
    """
    Run the API server using gunicorn with uvicorn workers.

    Gunicorn provides proper master/worker process management with:
    - SIGTTIN to spawn new workers
    - SIGTTOU to kill oldest worker (FIFO order - correct for rolling restarts)
    - Memory sharing via preload + fork copy-on-write

    Starts GunicornMonitor in a background thread to perform rolling worker restarts.
    """
    ssl_cert, ssl_key = _get_ssl_cert_and_key_filepaths(args)

    log_level = conf.get("logging", "uvicorn_logging_level", fallback="info").lower()
    access_log_enabled = log_level not in ("error", "critical", "fatal")

    cmd = _build_gunicorn_command(
        host=args.host,
        port=args.port,
        num_workers=num_workers,
        worker_timeout=worker_timeout,
        ssl_cert=ssl_cert,
        ssl_key=ssl_key,
        log_level=log_level,
        access_log_enabled=access_log_enabled,
        proxy_headers=proxy_headers,
    )

    log.info(
        textwrap.dedent(
            f"""\
            Running gunicorn with:
            Apps: {apps}
            Workers: {num_workers}
            Host: {args.host}:{args.port}
            Timeout: {worker_timeout}
            Command: {" ".join(cmd)}
            ================================================================="""
        )
    )

    gunicorn_proc = subprocess.Popen(cmd)

    def forward_signal(signum, frame):
        if gunicorn_proc.poll() is None:
            gunicorn_proc.send_signal(signum)

    original_sigterm = signal.signal(signal.SIGTERM, forward_signal)
    original_sigint = signal.signal(signal.SIGINT, forward_signal)

    try:
        worker_refresh_interval = conf.getint("api", "worker_refresh_interval", fallback=0)

        if worker_refresh_interval > 0:
            from airflow.cli.commands.gunicorn_monitor import create_monitor_from_config

            ssl_enabled = bool(ssl_cert and ssl_key)
            monitor = create_monitor_from_config(
                gunicorn_master_pid=gunicorn_proc.pid,
                num_workers=num_workers,
                host=args.host,
                port=args.port,
                ssl_enabled=ssl_enabled,
            )

            monitor_thread = threading.Thread(target=monitor.start, daemon=True)
            monitor_thread.start()
            log.info("Started GunicornMonitor thread for rolling worker restarts")

        return_code = gunicorn_proc.wait()
        if return_code != 0:
            log.error("Gunicorn exited with code %d", return_code)
            sys.exit(return_code)

    finally:
        signal.signal(signal.SIGTERM, original_sigterm)
        signal.signal(signal.SIGINT, original_sigint)

        if gunicorn_proc.poll() is None:
            gunicorn_proc.terminate()
            try:
                gunicorn_proc.wait(timeout=10)
            except subprocess.TimeoutExpired:
                gunicorn_proc.kill()


def _run_api_server_with_uvicorn(
    args,
    apps: str,
    num_workers: int,
    worker_timeout: int,
    proxy_headers: bool,
) -> None:
    """
    Run the API server using uvicorn directly.

    This is the default mode. Note that uvicorn's multiprocess mode does not
    share memory between workers (each worker loads everything independently).
    """
    ssl_cert, ssl_key = _get_ssl_cert_and_key_filepaths(args)

    # setproctitle causes issue on Mac OS: https://github.com/benoitc/gunicorn/issues/3021
    os_type = sys.platform
    if os_type == "darwin":
        log.debug("Mac OS detected, skipping setproctitle")
    else:
        from setproctitle import setproctitle

        setproctitle(f"airflow api_server -- host:{args.host} port:{args.port}")

    # Get uvicorn logging configuration from Airflow settings
    uvicorn_log_level = conf.get("logging", "uvicorn_logging_level", fallback="info").lower()
    # Control access log based on uvicorn log level - disable for ERROR and above
    access_log_enabled = uvicorn_log_level not in ("error", "critical", "fatal")

    uvicorn_kwargs = {
        "host": args.host,
        "port": args.port,
        "workers": num_workers,
        "timeout_keep_alive": worker_timeout,
        "timeout_graceful_shutdown": worker_timeout,
        "timeout_worker_healthcheck": worker_timeout,
        "ssl_keyfile": ssl_key,
        "ssl_certfile": ssl_cert,
        "access_log": access_log_enabled,
        "log_level": uvicorn_log_level,
        "proxy_headers": proxy_headers,
    }
    # Only set the log_config if it is provided, otherwise use the default uvicorn logging configuration.
    if args.log_config and args.log_config != "-":
        # The [api/log_config] is migrated from [api/access_logfile] and [api/access_logfile] defaults to "-" for stdout for Gunicorn.
        # So we need to check if the log_config is set to "-" or not; if it is set to "-", we regard it as not set.
        uvicorn_kwargs["log_config"] = args.log_config

    uvicorn.run(
        "airflow.api_fastapi.main:app",
        **uvicorn_kwargs,
    )


@enable_memray_trace(component=MemrayTraceComponents.api)
def _run_api_server(args, apps: str, num_workers: int, worker_timeout: int, proxy_headers: bool):
    """Run the API server using the configured server type."""
    server_type = conf.get("api", "server_type", fallback="uvicorn").lower()

    if server_type == "gunicorn":
        try:
            import gunicorn  # noqa: F401
        except ImportError:
            raise AirflowConfigException(
                "Gunicorn is not installed. Install it with: pip install 'apache-airflow-core[gunicorn]'"
            )

        log.info(
            textwrap.dedent(
                f"""\
                Running the API server with gunicorn:
                Apps: {apps}
                Workers: {num_workers}
                Host: {args.host}:{args.port}
                Timeout: {worker_timeout}
                Logfiles: {args.log_file or "-"}
                ================================================================="""
            )
        )
        _run_api_server_with_gunicorn(
            args=args,
            apps=apps,
            num_workers=num_workers,
            worker_timeout=worker_timeout,
            proxy_headers=proxy_headers,
        )
    else:
        log.info(
            textwrap.dedent(
                f"""\
                Running the API server with uvicorn:
                Apps: {apps}
                Workers: {num_workers}
                Host: {args.host}:{args.port}
                Timeout: {worker_timeout}
                Logfiles: {args.log_file or "-"}
                ================================================================="""
            )
        )
        _run_api_server_with_uvicorn(
            args=args,
            apps=apps,
            num_workers=num_workers,
            worker_timeout=worker_timeout,
            proxy_headers=proxy_headers,
        )


def with_api_apps_env(func: Callable[[Namespace], RT]) -> Callable[[Namespace], RT]:
    """We use AIRFLOW_API_APPS to specify which apps are initialized in the API server."""

    @wraps(func)
    def wrapper(args: Namespace) -> RT:
        apps: str = args.apps
        original_value = os.environ.get("AIRFLOW_API_APPS")
        try:
            log.debug("Setting AIRFLOW_API_APPS to: %s", apps)
            os.environ["AIRFLOW_API_APPS"] = apps
            return func(args)
        finally:
            if original_value is not None:
                os.environ["AIRFLOW_API_APPS"] = original_value
                log.debug("Restored AIRFLOW_API_APPS to: %s", original_value)
            else:
                os.environ.pop("AIRFLOW_API_APPS", None)
                log.debug("Removed AIRFLOW_API_APPS from environment")

    return wrapper


@cli_utils.action_cli
@providers_configuration_loaded
@with_api_apps_env
def api_server(args: Namespace):
    """Start Airflow API server."""
    print(settings.HEADER)

    apps = args.apps
    num_workers = args.workers
    worker_timeout = args.worker_timeout
    proxy_headers = args.proxy_headers

    # Ensure we set this now, so that each subprocess gets the same value
    from airflow.api_fastapi.auth.tokens import get_signing_args

    get_signing_args()

    if cli_utils.should_enable_hot_reload(args):
        print(f"Starting the API server on port {args.port} and host {args.host} in development mode.")
        log.warning("Running in dev mode, ignoring uvicorn args")
        from fastapi_cli.cli import _run

        _run(
            entrypoint="airflow.api_fastapi.main:app",
            port=args.port,
            host=args.host,
            reload=True,
            proxy_headers=args.proxy_headers,
            command="dev",
        )
        return

    run_command_with_daemon_option(
        args=args,
        process_name="api_server",
        callback=lambda: _run_api_server(
            args=args,
            apps=apps,
            num_workers=num_workers,
            worker_timeout=worker_timeout,
            proxy_headers=proxy_headers,
        ),
    )


def _get_ssl_cert_and_key_filepaths(cli_arguments) -> tuple[str | None, str | None]:
    error_template_1 = "Need both, have provided {} but not {}"
    error_template_2 = "SSL related file does not exist {}"

    ssl_cert, ssl_key = cli_arguments.ssl_cert, cli_arguments.ssl_key
    if ssl_cert and ssl_key:
        if not os.path.isfile(ssl_cert):
            raise AirflowConfigException(error_template_2.format(ssl_cert))
        if not os.path.isfile(ssl_key):
            raise AirflowConfigException(error_template_2.format(ssl_key))

        return (ssl_cert, ssl_key)
    if ssl_cert:
        raise AirflowConfigException(error_template_1.format("SSL certificate", "SSL key"))
    if ssl_key:
        raise AirflowConfigException(error_template_1.format("SSL key", "SSL certificate"))

    return (None, None)
